{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(t): # feed raw text in BERT so we will not preprocess\n",
    "    \n",
    "    sentence_maxlen = 0\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    all_favor_tweets = []\n",
    "    all_against_tweets = []\n",
    "    \n",
    "    with open(train_data_file, 'r') as trainfile:\n",
    "        for line in trainfile:\n",
    "            \n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "            \n",
    "            if line[0].strip() != 'ID' and line[3].strip() == 'FAVOR' and line[1] == t:\n",
    "                tweet = line[2]\n",
    "                #tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_favor_tweets.append(tweet)\n",
    "            elif line[0].strip() != 'ID' and line[3].strip() == 'AGAINST' and line[1] == t:\n",
    "                tweet = line[2]\n",
    "                #tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_against_tweets.append(tweet)\n",
    "            \n",
    "    x_train = all_favor_tweets + all_against_tweets\n",
    "    y_train = np.append(np.ones(len(all_favor_tweets)), np.zeros(len(all_against_tweets))) \n",
    "    \n",
    "    pp = len(x_train)\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    all_favor_tweets_test = []\n",
    "    all_against_tweets_test = []\n",
    "    with open(test_data_file, 'r') as testfile:\n",
    "        for line in testfile:\n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "        \n",
    "\n",
    "            if line[0] != 'ID' and line[3] == 'FAVOR' and line[1] == t:\n",
    "                tweet = line[2]\n",
    "                #tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_favor_tweets_test.append(tweet)\n",
    "            elif line[0].strip() != 'ID' and line[3].strip() == 'AGAINST' and line[1] == t:\n",
    "                tweet = line[2]\n",
    "                #tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_against_tweets_test.append(tweet)\n",
    "\n",
    "    x_test = all_favor_tweets_test + all_against_tweets_test\n",
    "    y_test = np.append(np.ones(len(all_favor_tweets_test)), np.zeros(len(all_against_tweets_test)))\n",
    "\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, sentence_maxlen, pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = '/home/parush/stance/Experiments/stance_mohammed/train.txt'\n",
    "test_data_file = '/home/parush/stance/Experiments/stance_mohammed/test.txt'\n",
    "TARGETS = [ 'Atheism','Climate Change is a Real Concern', 'Feminist Movement','Hillary Clinton', 'Legalization of Abortion' ]\n",
    "\n",
    "df = pd.read_csv(train_data_file, sep='\\t')\n",
    "df1 = pd.read_csv(test_data_file, sep='\\t')\n",
    "t = TARGETS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_file = '/home/parush/stance/Experiments/SomasundaranWiebe-politicalDebates/train.txt'\n",
    "# test_data_file = '/home/parush/stance/Experiments/SomasundaranWiebe-politicalDebates/test.txt'\n",
    "# TARGETS = ['god','healthcare','guns','gayRights','abortion', 'creation']\n",
    "# df = pd.read_csv(train_data_file, sep='\\t')\n",
    "# df1 = pd.read_csv(test_data_file, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_file = '/home/parush/stance/Experiments/Data_MPCHI/train.txt'\n",
    "# test_data_file = '/home/parush/stance/Experiments/Data_MPCHI/test.txt'\n",
    "# TARGETS = ['Are E-Cigarettes safe?','Does MMR Vaccine lead to autism in children?',\n",
    "#       'Does Sunlight exposure lead to skin cancer?','Does Vitamin C prevent common cold?',\n",
    "#       'Should women take HRT post-menopause?']\n",
    "# df = pd.read_csv(train_data_file, sep='\\t')\n",
    "# df1 = pd.read_csv(test_data_file, sep='\\t')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, test_texts, test_labels, sen_maxlen, pp = train_and_test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bert Classifier\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts)) #cannot be more than 512\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "\n",
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_y = np.array(train_labels) == 1\n",
    "test_y = np.array(test_labels) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # you can change it\n",
    "EPOCHS = 50 # do not train for too many epochs as a large model like this will surely overfit and overflow the ram and disk\n",
    "learning_rate = 2e-5 # you can decrease it more like 3e-5 but do not increase it more than 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is how you read data into Pytorch\n",
    "\n",
    "'''\n",
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/16.8125 loss: 0.7226166725158691 \n",
      "Epoch:  1\n",
      "1/16.8125 loss: 0.7084046900272369 \n",
      "Epoch:  1\n",
      "2/16.8125 loss: 0.6905917723973592 \n",
      "Epoch:  1\n",
      "3/16.8125 loss: 0.6734533905982971 \n",
      "Epoch:  1\n",
      "4/16.8125 loss: 0.6603018760681152 \n",
      "Epoch:  1\n",
      "5/16.8125 loss: 0.6631015539169312 \n",
      "Epoch:  1\n",
      "6/16.8125 loss: 0.6854724713734218 \n",
      "Epoch:  1\n",
      "7/16.8125 loss: 0.6815416514873505 \n",
      "Epoch:  1\n",
      "8/16.8125 loss: 0.6725233859486051 \n",
      "Epoch:  1\n",
      "9/16.8125 loss: 0.6732909500598907 \n",
      "Epoch:  1\n",
      "10/16.8125 loss: 0.67434669082815 \n",
      "Epoch:  1\n",
      "11/16.8125 loss: 0.6653572221597036 \n",
      "Epoch:  1\n",
      "12/16.8125 loss: 0.6636245663349445 \n",
      "Epoch:  1\n",
      "13/16.8125 loss: 0.6661753697054726 \n",
      "Epoch:  1\n",
      "14/16.8125 loss: 0.6684948603312174 \n",
      "Epoch:  1\n",
      "15/16.8125 loss: 0.6701503284275532 \n",
      "Epoch:  1\n",
      "16/16.8125 loss: 0.672521875185125 \n",
      "Epoch:  2\n",
      "0/16.8125 loss: 0.7038133144378662 \n",
      "Epoch:  2\n",
      "1/16.8125 loss: 0.6848434507846832 \n",
      "Epoch:  2\n",
      "2/16.8125 loss: 0.6613191564877828 \n",
      "Epoch:  2\n",
      "3/16.8125 loss: 0.6689949184656143 \n",
      "Epoch:  2\n",
      "4/16.8125 loss: 0.6774052619934082 \n",
      "Epoch:  2\n",
      "5/16.8125 loss: 0.671079158782959 \n",
      "Epoch:  2\n",
      "6/16.8125 loss: 0.6725669758660453 \n",
      "Epoch:  2\n",
      "7/16.8125 loss: 0.6696582213044167 \n",
      "Epoch:  2\n",
      "8/16.8125 loss: 0.6641194489267137 \n",
      "Epoch:  2\n",
      "9/16.8125 loss: 0.6558132112026215 \n",
      "Epoch:  2\n",
      "10/16.8125 loss: 0.6497103409333662 \n",
      "Epoch:  2\n",
      "11/16.8125 loss: 0.6524894684553146 \n",
      "Epoch:  2\n",
      "12/16.8125 loss: 0.6468989253044128 \n",
      "Epoch:  2\n",
      "13/16.8125 loss: 0.6449399888515472 \n",
      "Epoch:  2\n",
      "14/16.8125 loss: 0.6530073126157124 \n",
      "Epoch:  2\n",
      "15/16.8125 loss: 0.650826919823885 \n",
      "Epoch:  2\n",
      "16/16.8125 loss: 0.6506519457873177 \n",
      "Epoch:  3\n",
      "0/16.8125 loss: 0.6048958897590637 \n",
      "Epoch:  3\n",
      "1/16.8125 loss: 0.5973812639713287 \n",
      "Epoch:  3\n",
      "2/16.8125 loss: 0.6077902714411417 \n",
      "Epoch:  3\n",
      "3/16.8125 loss: 0.6122030019760132 \n",
      "Epoch:  3\n",
      "4/16.8125 loss: 0.605740225315094 \n",
      "Epoch:  3\n",
      "5/16.8125 loss: 0.5958580076694489 \n",
      "Epoch:  3\n",
      "6/16.8125 loss: 0.5876163414546421 \n",
      "Epoch:  3\n",
      "7/16.8125 loss: 0.5767770484089851 \n",
      "Epoch:  3\n",
      "8/16.8125 loss: 0.572071287367079 \n",
      "Epoch:  3\n",
      "9/16.8125 loss: 0.5845942378044129 \n",
      "Epoch:  3\n",
      "10/16.8125 loss: 0.581597544930198 \n",
      "Epoch:  3\n",
      "11/16.8125 loss: 0.594289094209671 \n",
      "Epoch:  3\n",
      "12/16.8125 loss: 0.6069533274723933 \n",
      "Epoch:  3\n",
      "13/16.8125 loss: 0.6109823840005058 \n",
      "Epoch:  3\n",
      "14/16.8125 loss: 0.6110500931739807 \n",
      "Epoch:  3\n",
      "15/16.8125 loss: 0.6064191684126854 \n",
      "Epoch:  3\n",
      "16/16.8125 loss: 0.6078457516782424 \n",
      "Epoch:  4\n",
      "0/16.8125 loss: 0.5450353026390076 \n",
      "Epoch:  4\n",
      "1/16.8125 loss: 0.5446932911872864 \n",
      "Epoch:  4\n",
      "2/16.8125 loss: 0.5267195006211599 \n",
      "Epoch:  4\n",
      "3/16.8125 loss: 0.5068585202097893 \n",
      "Epoch:  4\n",
      "4/16.8125 loss: 0.5134647905826568 \n",
      "Epoch:  4\n",
      "5/16.8125 loss: 0.5236067126194636 \n",
      "Epoch:  4\n",
      "6/16.8125 loss: 0.5164508691855839 \n",
      "Epoch:  4\n",
      "7/16.8125 loss: 0.5220922715961933 \n",
      "Epoch:  4\n",
      "8/16.8125 loss: 0.526291055811776 \n",
      "Epoch:  4\n",
      "9/16.8125 loss: 0.5339434593915939 \n",
      "Epoch:  4\n",
      "10/16.8125 loss: 0.5234340700236234 \n",
      "Epoch:  4\n",
      "11/16.8125 loss: 0.5186763207117716 \n",
      "Epoch:  4\n",
      "12/16.8125 loss: 0.5227149862509507 \n",
      "Epoch:  4\n",
      "13/16.8125 loss: 0.5240424914019448 \n",
      "Epoch:  4\n",
      "14/16.8125 loss: 0.5284845113754273 \n",
      "Epoch:  4\n",
      "15/16.8125 loss: 0.526748038828373 \n",
      "Epoch:  4\n",
      "16/16.8125 loss: 0.5212615009616403 \n",
      "Epoch:  5\n",
      "0/16.8125 loss: 0.417447566986084 \n",
      "Epoch:  5\n",
      "1/16.8125 loss: 0.4351327270269394 \n",
      "Epoch:  5\n",
      "2/16.8125 loss: 0.4396618604660034 \n",
      "Epoch:  5\n",
      "3/16.8125 loss: 0.4337983578443527 \n",
      "Epoch:  5\n",
      "4/16.8125 loss: 0.4248622417449951 \n",
      "Epoch:  5\n",
      "5/16.8125 loss: 0.41252275307973224 \n",
      "Epoch:  5\n",
      "6/16.8125 loss: 0.40419669236455646 \n",
      "Epoch:  5\n",
      "7/16.8125 loss: 0.41753337159752846 \n",
      "Epoch:  5\n",
      "8/16.8125 loss: 0.40312052104208207 \n",
      "Epoch:  5\n",
      "9/16.8125 loss: 0.4015907645225525 \n",
      "Epoch:  5\n",
      "10/16.8125 loss: 0.3872736692428589 \n",
      "Epoch:  5\n",
      "11/16.8125 loss: 0.39407460391521454 \n",
      "Epoch:  5\n",
      "12/16.8125 loss: 0.39195900696974534 \n",
      "Epoch:  5\n",
      "13/16.8125 loss: 0.398250760776656 \n",
      "Epoch:  5\n",
      "14/16.8125 loss: 0.3951416969299316 \n",
      "Epoch:  5\n",
      "15/16.8125 loss: 0.39606526494026184 \n",
      "Epoch:  5\n",
      "16/16.8125 loss: 0.3958721195950228 \n",
      "Epoch:  6\n",
      "0/16.8125 loss: 0.24398133158683777 \n",
      "Epoch:  6\n",
      "1/16.8125 loss: 0.25698259472846985 \n",
      "Epoch:  6\n",
      "2/16.8125 loss: 0.26455151041348773 \n",
      "Epoch:  6\n",
      "3/16.8125 loss: 0.2752610221505165 \n",
      "Epoch:  6\n",
      "4/16.8125 loss: 0.27049952149391177 \n",
      "Epoch:  6\n",
      "5/16.8125 loss: 0.27696317434310913 \n",
      "Epoch:  6\n",
      "6/16.8125 loss: 0.28166480149541584 \n",
      "Epoch:  6\n",
      "7/16.8125 loss: 0.2756864298135042 \n",
      "Epoch:  6\n",
      "8/16.8125 loss: 0.2755023390054703 \n",
      "Epoch:  6\n",
      "9/16.8125 loss: 0.2848267272114754 \n",
      "Epoch:  6\n",
      "10/16.8125 loss: 0.2868629572066394 \n",
      "Epoch:  6\n",
      "11/16.8125 loss: 0.2838428405423959 \n",
      "Epoch:  6\n",
      "12/16.8125 loss: 0.2761902167246892 \n",
      "Epoch:  6\n",
      "13/16.8125 loss: 0.27169073798826765 \n",
      "Epoch:  6\n",
      "14/16.8125 loss: 0.2647430509328842 \n",
      "Epoch:  6\n",
      "15/16.8125 loss: 0.2613669512793422 \n",
      "Epoch:  6\n",
      "16/16.8125 loss: 0.2609436836312799 \n",
      "Epoch:  7\n",
      "0/16.8125 loss: 0.3035542368888855 \n",
      "Epoch:  7\n",
      "1/16.8125 loss: 0.21160102263092995 \n",
      "Epoch:  7\n",
      "2/16.8125 loss: 0.23789319644371668 \n",
      "Epoch:  7\n",
      "3/16.8125 loss: 0.2518801484256983 \n",
      "Epoch:  7\n",
      "4/16.8125 loss: 0.23013601452112198 \n",
      "Epoch:  7\n",
      "5/16.8125 loss: 0.2214660607278347 \n",
      "Epoch:  7\n",
      "6/16.8125 loss: 0.2135690674185753 \n",
      "Epoch:  7\n",
      "7/16.8125 loss: 0.19731931667774916 \n",
      "Epoch:  7\n",
      "8/16.8125 loss: 0.1962334786852201 \n",
      "Epoch:  7\n",
      "9/16.8125 loss: 0.18591005429625512 \n",
      "Epoch:  7\n",
      "10/16.8125 loss: 0.18503063172101974 \n",
      "Epoch:  7\n",
      "11/16.8125 loss: 0.17654194434483847 \n",
      "Epoch:  7\n",
      "12/16.8125 loss: 0.18010123876424936 \n",
      "Epoch:  7\n",
      "13/16.8125 loss: 0.1825995487826211 \n",
      "Epoch:  7\n",
      "14/16.8125 loss: 0.17599144230286282 \n",
      "Epoch:  7\n",
      "15/16.8125 loss: 0.17394032003358006 \n",
      "Epoch:  7\n",
      "16/16.8125 loss: 0.16983933527680004 \n",
      "Epoch:  8\n",
      "0/16.8125 loss: 0.07395591586828232 \n",
      "Epoch:  8\n",
      "1/16.8125 loss: 0.20559554919600487 \n",
      "Epoch:  8\n",
      "2/16.8125 loss: 0.17178665101528168 \n",
      "Epoch:  8\n",
      "3/16.8125 loss: 0.16390139982104301 \n",
      "Epoch:  8\n",
      "4/16.8125 loss: 0.17828235626220704 \n",
      "Epoch:  8\n",
      "5/16.8125 loss: 0.17338835696379343 \n",
      "Epoch:  8\n",
      "6/16.8125 loss: 0.1639842348439353 \n",
      "Epoch:  8\n",
      "7/16.8125 loss: 0.14846602734178305 \n",
      "Epoch:  8\n",
      "8/16.8125 loss: 0.1454548959930738 \n",
      "Epoch:  8\n",
      "9/16.8125 loss: 0.13560011684894563 \n",
      "Epoch:  8\n",
      "10/16.8125 loss: 0.13109734518961472 \n",
      "Epoch:  8\n",
      "11/16.8125 loss: 0.12882359450062117 \n",
      "Epoch:  8\n",
      "12/16.8125 loss: 0.13300984754012182 \n",
      "Epoch:  8\n",
      "13/16.8125 loss: 0.12722081691026688 \n",
      "Epoch:  8\n",
      "14/16.8125 loss: 0.12167620013157526 \n",
      "Epoch:  8\n",
      "15/16.8125 loss: 0.117459318600595 \n",
      "Epoch:  8\n",
      "16/16.8125 loss: 0.11572839933283188 \n",
      "Epoch:  9\n",
      "0/16.8125 loss: 0.030632561072707176 \n",
      "Epoch:  9\n",
      "1/16.8125 loss: 0.1273480085656047 \n",
      "Epoch:  9\n",
      "2/16.8125 loss: 0.1096715151021878 \n",
      "Epoch:  9\n",
      "3/16.8125 loss: 0.11008671624585986 \n",
      "Epoch:  9\n",
      "4/16.8125 loss: 0.09404355511069298 \n",
      "Epoch:  9\n",
      "5/16.8125 loss: 0.08338301411519448 \n",
      "Epoch:  9\n",
      "6/16.8125 loss: 0.0855673426496131 \n",
      "Epoch:  9\n",
      "7/16.8125 loss: 0.08071308699436486 \n",
      "Epoch:  9\n",
      "8/16.8125 loss: 0.07751584197911951 \n",
      "Epoch:  9\n",
      "9/16.8125 loss: 0.07383826952427626 \n",
      "Epoch:  9\n",
      "10/16.8125 loss: 0.07394205287776211 \n",
      "Epoch:  9\n",
      "11/16.8125 loss: 0.06934123657022913 \n",
      "Epoch:  9\n",
      "12/16.8125 loss: 0.06635243660555436 \n",
      "Epoch:  9\n",
      "13/16.8125 loss: 0.06313334325594562 \n",
      "Epoch:  9\n",
      "14/16.8125 loss: 0.062192544092734656 \n",
      "Epoch:  9\n",
      "15/16.8125 loss: 0.06018513429444283 \n",
      "Epoch:  9\n",
      "16/16.8125 loss: 0.06058158583062537 \n",
      "Epoch:  10\n",
      "0/16.8125 loss: 0.017005419358611107 \n",
      "Epoch:  10\n",
      "1/16.8125 loss: 0.018172199837863445 \n",
      "Epoch:  10\n",
      "2/16.8125 loss: 0.018281772111852963 \n",
      "Epoch:  10\n",
      "3/16.8125 loss: 0.01902965549379587 \n",
      "Epoch:  10\n",
      "4/16.8125 loss: 0.019091898947954176 \n",
      "Epoch:  10\n",
      "5/16.8125 loss: 0.020945060377319653 \n",
      "Epoch:  10\n",
      "6/16.8125 loss: 0.02058605264340128 \n",
      "Epoch:  10\n",
      "7/16.8125 loss: 0.023998964577913284 \n",
      "Epoch:  10\n",
      "8/16.8125 loss: 0.026601675069994397 \n",
      "Epoch:  10\n",
      "9/16.8125 loss: 0.04075532667338848 \n",
      "Epoch:  10\n",
      "10/16.8125 loss: 0.03866249018094756 \n",
      "Epoch:  10\n",
      "11/16.8125 loss: 0.037949285469949245 \n",
      "Epoch:  10\n",
      "12/16.8125 loss: 0.03611094912943932 \n",
      "Epoch:  10\n",
      "13/16.8125 loss: 0.0399388181976974 \n",
      "Epoch:  10\n",
      "14/16.8125 loss: 0.04024691811452309 \n",
      "Epoch:  10\n",
      "15/16.8125 loss: 0.03886369202518836 \n",
      "Epoch:  10\n",
      "16/16.8125 loss: 0.04308405436356278 \n",
      "Epoch:  11\n",
      "0/16.8125 loss: 0.023606929928064346 \n",
      "Epoch:  11\n",
      "1/16.8125 loss: 0.01937789749354124 \n",
      "Epoch:  11\n",
      "2/16.8125 loss: 0.01842986543973287 \n",
      "Epoch:  11\n",
      "3/16.8125 loss: 0.016308136517181993 \n",
      "Epoch:  11\n",
      "4/16.8125 loss: 0.020383906550705433 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "5/16.8125 loss: 0.01942917937412858 \n",
      "Epoch:  11\n",
      "6/16.8125 loss: 0.018306035548448563 \n",
      "Epoch:  11\n",
      "7/16.8125 loss: 0.01750418997835368 \n",
      "Epoch:  11\n",
      "8/16.8125 loss: 0.02500794983158509 \n",
      "Epoch:  11\n",
      "9/16.8125 loss: 0.02388704353943467 \n",
      "Epoch:  11\n",
      "10/16.8125 loss: 0.022592080503024838 \n",
      "Epoch:  11\n",
      "11/16.8125 loss: 0.021826599957421422 \n",
      "Epoch:  11\n",
      "12/16.8125 loss: 0.021372513057520755 \n",
      "Epoch:  11\n",
      "13/16.8125 loss: 0.031027966245476688 \n",
      "Epoch:  11\n",
      "14/16.8125 loss: 0.029642668676873048 \n",
      "Epoch:  11\n",
      "15/16.8125 loss: 0.028995387081522495 \n",
      "Epoch:  11\n",
      "16/16.8125 loss: 0.028163699104505426 \n",
      "Epoch:  12\n",
      "0/16.8125 loss: 0.021810971200466156 \n",
      "Epoch:  12\n",
      "1/16.8125 loss: 0.014526840532198548 \n",
      "Epoch:  12\n",
      "2/16.8125 loss: 0.015512933488935232 \n",
      "Epoch:  12\n",
      "3/16.8125 loss: 0.014035990345291793 \n",
      "Epoch:  12\n",
      "4/16.8125 loss: 0.013175446446985007 \n",
      "Epoch:  12\n",
      "5/16.8125 loss: 0.01231213465022544 \n",
      "Epoch:  12\n",
      "6/16.8125 loss: 0.011536497888820512 \n",
      "Epoch:  12\n",
      "7/16.8125 loss: 0.011172746773809195 \n",
      "Epoch:  12\n",
      "8/16.8125 loss: 0.010817535842458407 \n",
      "Epoch:  12\n",
      "9/16.8125 loss: 0.010513087455183267 \n",
      "Epoch:  12\n",
      "10/16.8125 loss: 0.010507941923358223 \n",
      "Epoch:  12\n",
      "11/16.8125 loss: 0.010131747849906484 \n",
      "Epoch:  12\n",
      "12/16.8125 loss: 0.010427922225342346 \n",
      "Epoch:  12\n",
      "13/16.8125 loss: 0.010216008572440063 \n",
      "Epoch:  12\n",
      "14/16.8125 loss: 0.01889604947840174 \n",
      "Epoch:  12\n",
      "15/16.8125 loss: 0.0181411168014165 \n",
      "Epoch:  12\n",
      "16/16.8125 loss: 0.017456492354326388 \n",
      "Epoch:  13\n",
      "0/16.8125 loss: 0.006098415702581406 \n",
      "Epoch:  13\n",
      "1/16.8125 loss: 0.006138916127383709 \n",
      "Epoch:  13\n",
      "2/16.8125 loss: 0.0075735726083318395 \n",
      "Epoch:  13\n",
      "3/16.8125 loss: 0.007259214879013598 \n",
      "Epoch:  13\n",
      "4/16.8125 loss: 0.0070930544286966326 \n",
      "Epoch:  13\n",
      "5/16.8125 loss: 0.007404619517425696 \n",
      "Epoch:  13\n",
      "6/16.8125 loss: 0.007504672477287906 \n",
      "Epoch:  13\n",
      "7/16.8125 loss: 0.007744234753772616 \n",
      "Epoch:  13\n",
      "8/16.8125 loss: 0.007516083824965689 \n",
      "Epoch:  13\n",
      "9/16.8125 loss: 0.0073270469438284636 \n",
      "Epoch:  13\n",
      "10/16.8125 loss: 0.007207615047015927 \n",
      "Epoch:  13\n",
      "11/16.8125 loss: 0.008853216267501315 \n",
      "Epoch:  13\n",
      "12/16.8125 loss: 0.008586544185303725 \n",
      "Epoch:  13\n",
      "13/16.8125 loss: 0.008650796421404396 \n",
      "Epoch:  13\n",
      "14/16.8125 loss: 0.015272936411201954 \n",
      "Epoch:  13\n",
      "15/16.8125 loss: 0.014665490540210158 \n",
      "Epoch:  13\n",
      "16/16.8125 loss: 0.014118056443026838 \n",
      "Epoch:  14\n",
      "0/16.8125 loss: 0.006219417322427034 \n",
      "Epoch:  14\n",
      "1/16.8125 loss: 0.005435401340946555 \n",
      "Epoch:  14\n",
      "2/16.8125 loss: 0.005542165444542964 \n",
      "Epoch:  14\n",
      "3/16.8125 loss: 0.005668019992299378 \n",
      "Epoch:  14\n",
      "4/16.8125 loss: 0.005554317217320204 \n",
      "Epoch:  14\n",
      "5/16.8125 loss: 0.007697616315757235 \n",
      "Epoch:  14\n",
      "6/16.8125 loss: 0.007551719035421099 \n",
      "Epoch:  14\n",
      "7/16.8125 loss: 0.007205780420918018 \n",
      "Epoch:  14\n",
      "8/16.8125 loss: 0.00714073966567715 \n",
      "Epoch:  14\n",
      "9/16.8125 loss: 0.007020221231505275 \n",
      "Epoch:  14\n",
      "10/16.8125 loss: 0.006817280780524015 \n",
      "Epoch:  14\n",
      "11/16.8125 loss: 0.006622977399577697 \n",
      "Epoch:  14\n",
      "12/16.8125 loss: 0.017407703714875076 \n",
      "Epoch:  14\n",
      "13/16.8125 loss: 0.017929839502487863 \n",
      "Epoch:  14\n",
      "14/16.8125 loss: 0.017423252513011297 \n",
      "Epoch:  14\n",
      "15/16.8125 loss: 0.017185662349220365 \n",
      "Epoch:  14\n",
      "16/16.8125 loss: 0.02023943026057061 \n",
      "Epoch:  15\n",
      "0/16.8125 loss: 0.1123172789812088 \n",
      "Epoch:  15\n",
      "1/16.8125 loss: 0.06077704019844532 \n",
      "Epoch:  15\n",
      "2/16.8125 loss: 0.044667751217881836 \n",
      "Epoch:  15\n",
      "3/16.8125 loss: 0.03456683305557817 \n",
      "Epoch:  15\n",
      "4/16.8125 loss: 0.02995667764917016 \n",
      "Epoch:  15\n",
      "5/16.8125 loss: 0.028320883788789313 \n",
      "Epoch:  15\n",
      "6/16.8125 loss: 0.0249683721922338 \n",
      "Epoch:  15\n",
      "7/16.8125 loss: 0.025033264246303588 \n",
      "Epoch:  15\n",
      "8/16.8125 loss: 0.022815978309760492 \n",
      "Epoch:  15\n",
      "9/16.8125 loss: 0.04024804006330669 \n",
      "Epoch:  15\n",
      "10/16.8125 loss: 0.036993551195006476 \n",
      "Epoch:  15\n",
      "11/16.8125 loss: 0.0343048139475286 \n",
      "Epoch:  15\n",
      "12/16.8125 loss: 0.03209179012964551 \n",
      "Epoch:  15\n",
      "13/16.8125 loss: 0.030359256713251983 \n",
      "Epoch:  15\n",
      "14/16.8125 loss: 0.028802958223968743 \n",
      "Epoch:  15\n",
      "15/16.8125 loss: 0.02738511873758398 \n",
      "Epoch:  15\n",
      "16/16.8125 loss: 0.02600522109252565 \n",
      "Epoch:  16\n",
      "0/16.8125 loss: 0.10232005268335342 \n",
      "Epoch:  16\n",
      "1/16.8125 loss: 0.05320861283689737 \n",
      "Epoch:  16\n",
      "2/16.8125 loss: 0.03688536863774061 \n",
      "Epoch:  16\n",
      "3/16.8125 loss: 0.028730495483614504 \n",
      "Epoch:  16\n",
      "4/16.8125 loss: 0.06088134841993451 \n",
      "Epoch:  16\n",
      "5/16.8125 loss: 0.05181674255679051 \n",
      "Epoch:  16\n",
      "6/16.8125 loss: 0.04606634604611567 \n",
      "Epoch:  16\n",
      "7/16.8125 loss: 0.04080167115898803 \n",
      "Epoch:  16\n",
      "8/16.8125 loss: 0.036784743372764855 \n",
      "Epoch:  16\n",
      "9/16.8125 loss: 0.03410053951665759 \n",
      "Epoch:  16\n",
      "10/16.8125 loss: 0.03210723789578134 \n",
      "Epoch:  16\n",
      "11/16.8125 loss: 0.03477461558456222 \n",
      "Epoch:  16\n",
      "12/16.8125 loss: 0.032656979317275375 \n",
      "Epoch:  16\n",
      "13/16.8125 loss: 0.031017110150839602 \n",
      "Epoch:  16\n",
      "14/16.8125 loss: 0.029822349175810815 \n",
      "Epoch:  16\n",
      "15/16.8125 loss: 0.028292931616306305 \n",
      "Epoch:  16\n",
      "16/16.8125 loss: 0.026977771893143654 \n",
      "Epoch:  17\n",
      "0/16.8125 loss: 0.004601844120770693 \n",
      "Epoch:  17\n",
      "1/16.8125 loss: 0.0050170524045825005 \n",
      "Epoch:  17\n",
      "2/16.8125 loss: 0.005016840839137633 \n",
      "Epoch:  17\n",
      "3/16.8125 loss: 0.004743912606500089 \n",
      "Epoch:  17\n",
      "4/16.8125 loss: 0.004581296164542436 \n",
      "Epoch:  17\n",
      "5/16.8125 loss: 0.004393663334970673 \n",
      "Epoch:  17\n",
      "6/16.8125 loss: 0.02117427658023579 \n",
      "Epoch:  17\n",
      "7/16.8125 loss: 0.019045122026000172 \n",
      "Epoch:  17\n",
      "8/16.8125 loss: 0.017436818064500887 \n",
      "Epoch:  17\n",
      "9/16.8125 loss: 0.016356739914044738 \n",
      "Epoch:  17\n",
      "10/16.8125 loss: 0.015233614270321348 \n",
      "Epoch:  17\n",
      "11/16.8125 loss: 0.014806742353054384 \n",
      "Epoch:  17\n",
      "12/16.8125 loss: 0.01420070517521638 \n",
      "Epoch:  17\n",
      "13/16.8125 loss: 0.014646042670522417 \n",
      "Epoch:  17\n",
      "14/16.8125 loss: 0.014006472751498222 \n",
      "Epoch:  17\n",
      "15/16.8125 loss: 0.01356086740270257 \n",
      "Epoch:  17\n",
      "16/16.8125 loss: 0.015147221877294429 \n",
      "Epoch:  18\n",
      "0/16.8125 loss: 0.005537151824682951 \n",
      "Epoch:  18\n",
      "1/16.8125 loss: 0.004488663282245398 \n",
      "Epoch:  18\n",
      "2/16.8125 loss: 0.012444463558495045 \n",
      "Epoch:  18\n",
      "3/16.8125 loss: 0.018390504410490394 \n",
      "Epoch:  18\n",
      "4/16.8125 loss: 0.01573378499597311 \n",
      "Epoch:  18\n",
      "5/16.8125 loss: 0.0136255226098001 \n",
      "Epoch:  18\n",
      "6/16.8125 loss: 0.012236959034843104 \n",
      "Epoch:  18\n",
      "7/16.8125 loss: 0.01107777634751983 \n",
      "Epoch:  18\n",
      "8/16.8125 loss: 0.011917589051235054 \n",
      "Epoch:  18\n",
      "9/16.8125 loss: 0.011154819489456713 \n",
      "Epoch:  18\n",
      "10/16.8125 loss: 0.010518203087320382 \n",
      "Epoch:  18\n",
      "11/16.8125 loss: 0.010077116797522953 \n",
      "Epoch:  18\n",
      "12/16.8125 loss: 0.009553687252963964 \n",
      "Epoch:  18\n",
      "13/16.8125 loss: 0.011415706714615226 \n",
      "Epoch:  18\n",
      "14/16.8125 loss: 0.010914782574400306 \n",
      "Epoch:  18\n",
      "15/16.8125 loss: 0.010963427324895747 \n",
      "Epoch:  18\n",
      "16/16.8125 loss: 0.01052771053989144 \n",
      "Epoch:  19\n",
      "0/16.8125 loss: 0.002834990620613098 \n",
      "Epoch:  19\n",
      "1/16.8125 loss: 0.04538131505250931 \n",
      "Epoch:  19\n",
      "2/16.8125 loss: 0.031124438159167767 \n",
      "Epoch:  19\n",
      "3/16.8125 loss: 0.024321856908500195 \n",
      "Epoch:  19\n",
      "4/16.8125 loss: 0.020090613793581725 \n",
      "Epoch:  19\n",
      "5/16.8125 loss: 0.01943227485753596 \n",
      "Epoch:  19\n",
      "6/16.8125 loss: 0.025894735208047286 \n",
      "Epoch:  19\n",
      "7/16.8125 loss: 0.026437419175636023 \n",
      "Epoch:  19\n",
      "8/16.8125 loss: 0.02379560563713312 \n",
      "Epoch:  19\n",
      "9/16.8125 loss: 0.0273313713259995 \n",
      "Epoch:  19\n",
      "10/16.8125 loss: 0.025353772396391087 \n",
      "Epoch:  19\n",
      "11/16.8125 loss: 0.023543842680131395 \n",
      "Epoch:  19\n",
      "12/16.8125 loss: 0.023712207276660662 \n",
      "Epoch:  19\n",
      "13/16.8125 loss: 0.02219988646850522 \n",
      "Epoch:  19\n",
      "14/16.8125 loss: 0.021579665582006177 \n",
      "Epoch:  19\n",
      "15/16.8125 loss: 0.020459009625483304 \n",
      "Epoch:  19\n",
      "16/16.8125 loss: 0.01951814941404497 \n",
      "Epoch:  20\n",
      "0/16.8125 loss: 0.005660137627273798 \n",
      "Epoch:  20\n",
      "1/16.8125 loss: 0.004075565026141703 \n",
      "Epoch:  20\n",
      "2/16.8125 loss: 0.0036785983635733524 \n",
      "Epoch:  20\n",
      "3/16.8125 loss: 0.0037564749945886433 \n",
      "Epoch:  20\n",
      "4/16.8125 loss: 0.003725070832297206 \n",
      "Epoch:  20\n",
      "5/16.8125 loss: 0.012381057837046683 \n",
      "Epoch:  20\n",
      "6/16.8125 loss: 0.011947633000090718 \n",
      "Epoch:  20\n",
      "7/16.8125 loss: 0.011446234042523429 \n",
      "Epoch:  20\n",
      "8/16.8125 loss: 0.010511577181104157 \n",
      "Epoch:  20\n",
      "9/16.8125 loss: 0.009832334495149553 \n",
      "Epoch:  20\n",
      "10/16.8125 loss: 0.02118583156896586 \n",
      "Epoch:  20\n",
      "11/16.8125 loss: 0.019889398458569 \n",
      "Epoch:  20\n",
      "12/16.8125 loss: 0.018677319668663237 \n",
      "Epoch:  20\n",
      "13/16.8125 loss: 0.01798000098538718 \n",
      "Epoch:  20\n",
      "14/16.8125 loss: 0.017187570796037714 \n",
      "Epoch:  20\n",
      "15/16.8125 loss: 0.0164022446697345 \n",
      "Epoch:  20\n",
      "16/16.8125 loss: 0.01563820485299563 \n",
      "Epoch:  21\n",
      "0/16.8125 loss: 0.004474282264709473 \n",
      "Epoch:  21\n",
      "1/16.8125 loss: 0.003922215662896633 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21\n",
      "2/16.8125 loss: 0.003500269182647268 \n",
      "Epoch:  21\n",
      "3/16.8125 loss: 0.003448602801654488 \n",
      "Epoch:  21\n",
      "4/16.8125 loss: 0.01009344165213406 \n",
      "Epoch:  21\n",
      "5/16.8125 loss: 0.009177699801512063 \n",
      "Epoch:  21\n",
      "6/16.8125 loss: 0.010313426643343908 \n",
      "Epoch:  21\n",
      "7/16.8125 loss: 0.02442680750391446 \n",
      "Epoch:  21\n",
      "8/16.8125 loss: 0.022236387426447537 \n",
      "Epoch:  21\n",
      "9/16.8125 loss: 0.020524108200334013 \n",
      "Epoch:  21\n",
      "10/16.8125 loss: 0.018892734544351697 \n",
      "Epoch:  21\n",
      "11/16.8125 loss: 0.018403335144588102 \n",
      "Epoch:  21\n",
      "12/16.8125 loss: 0.017264886723401453 \n",
      "Epoch:  21\n",
      "13/16.8125 loss: 0.0163341533791806 \n",
      "Epoch:  21\n",
      "14/16.8125 loss: 0.015498943983887632 \n",
      "Epoch:  21\n",
      "15/16.8125 loss: 0.014782163532800041 \n",
      "Epoch:  21\n",
      "16/16.8125 loss: 0.014216104316908647 \n",
      "Epoch:  22\n",
      "0/16.8125 loss: 0.002517560264095664 \n",
      "Epoch:  22\n",
      "1/16.8125 loss: 0.0030064332531765103 \n",
      "Epoch:  22\n",
      "2/16.8125 loss: 0.018017044368510444 \n",
      "Epoch:  22\n",
      "3/16.8125 loss: 0.014162712555844337 \n",
      "Epoch:  22\n",
      "4/16.8125 loss: 0.011849951930344105 \n",
      "Epoch:  22\n",
      "5/16.8125 loss: 0.011724236576507488 \n",
      "Epoch:  22\n",
      "6/16.8125 loss: 0.04248671912189041 \n",
      "Epoch:  22\n",
      "7/16.8125 loss: 0.03875904809683561 \n",
      "Epoch:  22\n",
      "8/16.8125 loss: 0.03486058545402355 \n",
      "Epoch:  22\n",
      "9/16.8125 loss: 0.031636793678626415 \n",
      "Epoch:  22\n",
      "10/16.8125 loss: 0.028993441003628752 \n",
      "Epoch:  22\n",
      "11/16.8125 loss: 0.026865075206539284 \n",
      "Epoch:  22\n",
      "12/16.8125 loss: 0.02497862374338393 \n",
      "Epoch:  22\n",
      "13/16.8125 loss: 0.023441302756379758 \n",
      "Epoch:  22\n",
      "14/16.8125 loss: 0.022090777444342773 \n",
      "Epoch:  22\n",
      "15/16.8125 loss: 0.025198895658832043 \n",
      "Epoch:  22\n",
      "16/16.8125 loss: 0.02396604448885602 \n",
      "Epoch:  23\n",
      "0/16.8125 loss: 0.002642891136929393 \n",
      "Epoch:  23\n",
      "1/16.8125 loss: 0.004383420920930803 \n",
      "Epoch:  23\n",
      "2/16.8125 loss: 0.003957236108059685 \n",
      "Epoch:  23\n",
      "3/16.8125 loss: 0.003872323897667229 \n",
      "Epoch:  23\n",
      "4/16.8125 loss: 0.0037252889946103097 \n",
      "Epoch:  23\n",
      "5/16.8125 loss: 0.004127862475191553 \n",
      "Epoch:  23\n",
      "6/16.8125 loss: 0.00396024325995573 \n",
      "Epoch:  23\n",
      "7/16.8125 loss: 0.004045540379593149 \n",
      "Epoch:  23\n",
      "8/16.8125 loss: 0.004032601266064578 \n",
      "Epoch:  23\n",
      "9/16.8125 loss: 0.009686963423155249 \n",
      "Epoch:  23\n",
      "10/16.8125 loss: 0.009178415258330378 \n",
      "Epoch:  23\n",
      "11/16.8125 loss: 0.008867448952514678 \n",
      "Epoch:  23\n",
      "12/16.8125 loss: 0.008430723441191591 \n",
      "Epoch:  23\n",
      "13/16.8125 loss: 0.008077763154038362 \n",
      "Epoch:  23\n",
      "14/16.8125 loss: 0.00810422661403815 \n",
      "Epoch:  23\n",
      "15/16.8125 loss: 0.008590977056883276 \n",
      "Epoch:  23\n",
      "16/16.8125 loss: 0.010966487557572476 \n",
      "Epoch:  24\n",
      "0/16.8125 loss: 0.10569114983081818 \n",
      "Epoch:  24\n",
      "1/16.8125 loss: 0.05396850628312677 \n",
      "Epoch:  24\n",
      "2/16.8125 loss: 0.03681605813714365 \n",
      "Epoch:  24\n",
      "3/16.8125 loss: 0.028929094842169434 \n",
      "Epoch:  24\n",
      "4/16.8125 loss: 0.023569236509501934 \n",
      "Epoch:  24\n",
      "5/16.8125 loss: 0.020194730488583446 \n",
      "Epoch:  24\n",
      "6/16.8125 loss: 0.018271751768354858 \n",
      "Epoch:  24\n",
      "7/16.8125 loss: 0.016448542912257835 \n",
      "Epoch:  24\n",
      "8/16.8125 loss: 0.016020014649257064 \n",
      "Epoch:  24\n",
      "9/16.8125 loss: 0.014955233712680639 \n",
      "Epoch:  24\n",
      "10/16.8125 loss: 0.014770893147215247 \n",
      "Epoch:  24\n",
      "11/16.8125 loss: 0.013810939776400724 \n",
      "Epoch:  24\n",
      "12/16.8125 loss: 0.01991826668381691 \n",
      "Epoch:  24\n",
      "13/16.8125 loss: 0.01866855567121612 \n",
      "Epoch:  24\n",
      "14/16.8125 loss: 0.02208366432848076 \n",
      "Epoch:  24\n",
      "15/16.8125 loss: 0.020829560337006114 \n",
      "Epoch:  24\n",
      "16/16.8125 loss: 0.019869455478756744 \n",
      "Epoch:  25\n",
      "0/16.8125 loss: 0.2936808168888092 \n",
      "Epoch:  25\n",
      "1/16.8125 loss: 0.2170776203274727 \n",
      "Epoch:  25\n",
      "2/16.8125 loss: 0.14760616359611353 \n",
      "Epoch:  25\n",
      "3/16.8125 loss: 0.1117555849486962 \n",
      "Epoch:  25\n",
      "4/16.8125 loss: 0.08989558084867895 \n",
      "Epoch:  25\n",
      "5/16.8125 loss: 0.07752170394329976 \n",
      "Epoch:  25\n",
      "6/16.8125 loss: 0.07994624955712684 \n",
      "Epoch:  25\n",
      "7/16.8125 loss: 0.07299773380509578 \n",
      "Epoch:  25\n",
      "8/16.8125 loss: 0.06548052242336173 \n",
      "Epoch:  25\n",
      "9/16.8125 loss: 0.05959564216900617 \n",
      "Epoch:  25\n",
      "10/16.8125 loss: 0.05442921167493544 \n",
      "Epoch:  25\n",
      "11/16.8125 loss: 0.05017548925631369 \n",
      "Epoch:  25\n",
      "12/16.8125 loss: 0.046614206091572456 \n",
      "Epoch:  25\n",
      "13/16.8125 loss: 0.043448639468156865 \n",
      "Epoch:  25\n",
      "14/16.8125 loss: 0.04258034623538454 \n",
      "Epoch:  25\n",
      "15/16.8125 loss: 0.04285006594727747 \n",
      "Epoch:  25\n",
      "16/16.8125 loss: 0.04049974021173137 \n",
      "Epoch:  26\n",
      "0/16.8125 loss: 0.0031259111128747463 \n",
      "Epoch:  26\n",
      "1/16.8125 loss: 0.0025469508254900575 \n",
      "Epoch:  26\n",
      "2/16.8125 loss: 0.0026965910413612923 \n",
      "Epoch:  26\n",
      "3/16.8125 loss: 0.008598919434007257 \n",
      "Epoch:  26\n",
      "4/16.8125 loss: 0.007499738177284598 \n",
      "Epoch:  26\n",
      "5/16.8125 loss: 0.006730329788600405 \n",
      "Epoch:  26\n",
      "6/16.8125 loss: 0.006071899285806077 \n",
      "Epoch:  26\n",
      "7/16.8125 loss: 0.005684794101398438 \n",
      "Epoch:  26\n",
      "8/16.8125 loss: 0.005592531576338742 \n",
      "Epoch:  26\n",
      "9/16.8125 loss: 0.005302199744619429 \n",
      "Epoch:  26\n",
      "10/16.8125 loss: 0.005063965450972319 \n",
      "Epoch:  26\n",
      "11/16.8125 loss: 0.004882711044047028 \n",
      "Epoch:  26\n",
      "12/16.8125 loss: 0.004698987118899822 \n",
      "Epoch:  26\n",
      "13/16.8125 loss: 0.004524075875191816 \n",
      "Epoch:  26\n",
      "14/16.8125 loss: 0.004863898813103636 \n",
      "Epoch:  26\n",
      "15/16.8125 loss: 0.004786164005054161 \n",
      "Epoch:  26\n",
      "16/16.8125 loss: 0.004671353025033194 \n",
      "Epoch:  27\n",
      "0/16.8125 loss: 0.002037930767983198 \n",
      "Epoch:  27\n",
      "1/16.8125 loss: 0.002588095492683351 \n",
      "Epoch:  27\n",
      "2/16.8125 loss: 0.026603576028719544 \n",
      "Epoch:  27\n",
      "3/16.8125 loss: 0.02041354522225447 \n",
      "Epoch:  27\n",
      "4/16.8125 loss: 0.01682301361579448 \n",
      "Epoch:  27\n",
      "5/16.8125 loss: 0.016052706729775917 \n",
      "Epoch:  27\n",
      "6/16.8125 loss: 0.015121041747209216 \n",
      "Epoch:  27\n",
      "7/16.8125 loss: 0.015196190288406797 \n",
      "Epoch:  27\n",
      "8/16.8125 loss: 0.01404704874019242 \n",
      "Epoch:  27\n",
      "9/16.8125 loss: 0.015156963805202395 \n",
      "Epoch:  27\n",
      "10/16.8125 loss: 0.014024539205076342 \n",
      "Epoch:  27\n",
      "11/16.8125 loss: 0.012985285410347084 \n",
      "Epoch:  27\n",
      "12/16.8125 loss: 0.012224421764795598 \n",
      "Epoch:  27\n",
      "13/16.8125 loss: 0.011839443119242787 \n",
      "Epoch:  27\n",
      "14/16.8125 loss: 0.011175696629409988 \n",
      "Epoch:  27\n",
      "15/16.8125 loss: 0.01063000230351463 \n",
      "Epoch:  27\n",
      "16/16.8125 loss: 0.010124394108596094 \n",
      "Epoch:  28\n",
      "0/16.8125 loss: 0.0026284947525709867 \n",
      "Epoch:  28\n",
      "1/16.8125 loss: 0.002359827165491879 \n",
      "Epoch:  28\n",
      "2/16.8125 loss: 0.002992395699645082 \n",
      "Epoch:  28\n",
      "3/16.8125 loss: 0.002738998970016837 \n",
      "Epoch:  28\n",
      "4/16.8125 loss: 0.0026827291119843723 \n",
      "Epoch:  28\n",
      "5/16.8125 loss: 0.0027711845080678663 \n",
      "Epoch:  28\n",
      "6/16.8125 loss: 0.002608119426960392 \n",
      "Epoch:  28\n",
      "7/16.8125 loss: 0.0025738941767485812 \n",
      "Epoch:  28\n",
      "8/16.8125 loss: 0.0025010623374126023 \n",
      "Epoch:  28\n",
      "9/16.8125 loss: 0.0048049011384136975 \n",
      "Epoch:  28\n",
      "10/16.8125 loss: 0.005155412744815377 \n",
      "Epoch:  28\n",
      "11/16.8125 loss: 0.004832340666325763 \n",
      "Epoch:  28\n",
      "12/16.8125 loss: 0.004586267177588665 \n",
      "Epoch:  28\n",
      "13/16.8125 loss: 0.004363986655204956 \n",
      "Epoch:  28\n",
      "14/16.8125 loss: 0.004175176657736301 \n",
      "Epoch:  28\n",
      "15/16.8125 loss: 0.0040032397519098595 \n",
      "Epoch:  28\n",
      "16/16.8125 loss: 0.003946058904094731 \n",
      "Epoch:  29\n",
      "0/16.8125 loss: 0.0033022041898220778 \n",
      "Epoch:  29\n",
      "1/16.8125 loss: 0.002787811099551618 \n",
      "Epoch:  29\n",
      "2/16.8125 loss: 0.003051439765840769 \n",
      "Epoch:  29\n",
      "3/16.8125 loss: 0.0035441757645457983 \n",
      "Epoch:  29\n",
      "4/16.8125 loss: 0.036134095303714274 \n",
      "Epoch:  29\n",
      "5/16.8125 loss: 0.03347944080208739 \n",
      "Epoch:  29\n",
      "6/16.8125 loss: 0.03396542676325355 \n",
      "Epoch:  29\n",
      "7/16.8125 loss: 0.02990007697371766 \n",
      "Epoch:  29\n",
      "8/16.8125 loss: 0.02682234077817864 \n",
      "Epoch:  29\n",
      "9/16.8125 loss: 0.024373503774404524 \n",
      "Epoch:  29\n",
      "10/16.8125 loss: 0.04404064945199273 \n",
      "Epoch:  29\n",
      "11/16.8125 loss: 0.04352691179762284 \n",
      "Epoch:  29\n",
      "12/16.8125 loss: 0.04116275791938488 \n",
      "Epoch:  29\n",
      "13/16.8125 loss: 0.03841761330009571 \n",
      "Epoch:  29\n",
      "14/16.8125 loss: 0.0360270823041598 \n",
      "Epoch:  29\n",
      "15/16.8125 loss: 0.040385175962001085 \n",
      "Epoch:  29\n",
      "16/16.8125 loss: 0.03814301919192076 \n",
      "Epoch:  30\n",
      "0/16.8125 loss: 0.05853001028299332 \n",
      "Epoch:  30\n",
      "1/16.8125 loss: 0.03209877014160156 \n",
      "Epoch:  30\n",
      "2/16.8125 loss: 0.022470981270695727 \n",
      "Epoch:  30\n",
      "3/16.8125 loss: 0.0174406161531806 \n",
      "Epoch:  30\n",
      "4/16.8125 loss: 0.015852427296340465 \n",
      "Epoch:  30\n",
      "5/16.8125 loss: 0.013854605262167752 \n",
      "Epoch:  30\n",
      "6/16.8125 loss: 0.012349514257428902 \n",
      "Epoch:  30\n",
      "7/16.8125 loss: 0.011230870295548812 \n",
      "Epoch:  30\n",
      "8/16.8125 loss: 0.01084260251890454 \n",
      "Epoch:  30\n",
      "9/16.8125 loss: 0.010164002166129648 \n",
      "Epoch:  30\n",
      "10/16.8125 loss: 0.017296146114610812 \n",
      "Epoch:  30\n",
      "11/16.8125 loss: 0.017096147426248837 \n",
      "Epoch:  30\n",
      "12/16.8125 loss: 0.016606432618573308 \n",
      "Epoch:  30\n",
      "13/16.8125 loss: 0.01732131201840405 \n",
      "Epoch:  30\n",
      "14/16.8125 loss: 0.017375617862368624 \n",
      "Epoch:  30\n",
      "15/16.8125 loss: 0.01651545541244559 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  30\n",
      "16/16.8125 loss: 0.015697878169114974 \n",
      "Epoch:  31\n",
      "0/16.8125 loss: 0.003362893359735608 \n",
      "Epoch:  31\n",
      "1/16.8125 loss: 0.00423407054040581 \n",
      "Epoch:  31\n",
      "2/16.8125 loss: 0.0037265519301096597 \n",
      "Epoch:  31\n",
      "3/16.8125 loss: 0.0033602953772060573 \n",
      "Epoch:  31\n",
      "4/16.8125 loss: 0.00317580453120172 \n",
      "Epoch:  31\n",
      "5/16.8125 loss: 0.002944354860422512 \n",
      "Epoch:  31\n",
      "6/16.8125 loss: 0.0027345974397446427 \n",
      "Epoch:  31\n",
      "7/16.8125 loss: 0.00260696055192966 \n",
      "Epoch:  31\n",
      "8/16.8125 loss: 0.0029440515079639023 \n",
      "Epoch:  31\n",
      "9/16.8125 loss: 0.0028201002511195838 \n",
      "Epoch:  31\n",
      "10/16.8125 loss: 0.0030187914380803704 \n",
      "Epoch:  31\n",
      "11/16.8125 loss: 0.002887096459744498 \n",
      "Epoch:  31\n",
      "12/16.8125 loss: 0.0028318424189749817 \n",
      "Epoch:  31\n",
      "13/16.8125 loss: 0.0027938730199821293 \n",
      "Epoch:  31\n",
      "14/16.8125 loss: 0.0026928597129881383 \n",
      "Epoch:  31\n",
      "15/16.8125 loss: 0.0030285085958894342 \n",
      "Epoch:  31\n",
      "16/16.8125 loss: 0.0029388538917855304 \n",
      "Epoch:  32\n",
      "0/16.8125 loss: 0.0012515936978161335 \n",
      "Epoch:  32\n",
      "1/16.8125 loss: 0.0016426232177764177 \n",
      "Epoch:  32\n",
      "2/16.8125 loss: 0.0015691212999324005 \n",
      "Epoch:  32\n",
      "3/16.8125 loss: 0.0014994365919847041 \n",
      "Epoch:  32\n",
      "4/16.8125 loss: 0.0016156981466338039 \n",
      "Epoch:  32\n",
      "5/16.8125 loss: 0.0015601803897880018 \n",
      "Epoch:  32\n",
      "6/16.8125 loss: 0.0015960066812112927 \n",
      "Epoch:  32\n",
      "7/16.8125 loss: 0.0019604782428359613 \n",
      "Epoch:  32\n",
      "8/16.8125 loss: 0.001922903297882941 \n",
      "Epoch:  32\n",
      "9/16.8125 loss: 0.001874326542019844 \n",
      "Epoch:  32\n",
      "10/16.8125 loss: 0.0018155553208833392 \n",
      "Epoch:  32\n",
      "11/16.8125 loss: 0.0017791282540808122 \n",
      "Epoch:  32\n",
      "12/16.8125 loss: 0.0025366139239989794 \n",
      "Epoch:  32\n",
      "13/16.8125 loss: 0.0026344773692211936 \n",
      "Epoch:  32\n",
      "14/16.8125 loss: 0.002536255633458495 \n",
      "Epoch:  32\n",
      "15/16.8125 loss: 0.002445258680381812 \n",
      "Epoch:  32\n",
      "16/16.8125 loss: 0.002380007556091775 \n",
      "Epoch:  33\n",
      "0/16.8125 loss: 0.001252398476935923 \n",
      "Epoch:  33\n",
      "1/16.8125 loss: 0.001499016652815044 \n",
      "Epoch:  33\n",
      "2/16.8125 loss: 0.0018333250191062689 \n",
      "Epoch:  33\n",
      "3/16.8125 loss: 0.001673957274761051 \n",
      "Epoch:  33\n",
      "4/16.8125 loss: 0.0015636488329619168 \n",
      "Epoch:  33\n",
      "5/16.8125 loss: 0.0015167953097261488 \n",
      "Epoch:  33\n",
      "6/16.8125 loss: 0.0014646112353407911 \n",
      "Epoch:  33\n",
      "7/16.8125 loss: 0.0014330183621495962 \n",
      "Epoch:  33\n",
      "8/16.8125 loss: 0.0014116451299438875 \n",
      "Epoch:  33\n",
      "9/16.8125 loss: 0.0014209563843905926 \n",
      "Epoch:  33\n",
      "10/16.8125 loss: 0.0014276639029214327 \n",
      "Epoch:  33\n",
      "11/16.8125 loss: 0.001428767815620328 \n",
      "Epoch:  33\n",
      "12/16.8125 loss: 0.0014423150210999525 \n",
      "Epoch:  33\n",
      "13/16.8125 loss: 0.0014273722024102295 \n",
      "Epoch:  33\n",
      "14/16.8125 loss: 0.0014126460378368695 \n",
      "Epoch:  33\n",
      "15/16.8125 loss: 0.001798652607249096 \n",
      "Epoch:  33\n",
      "16/16.8125 loss: 0.0017668952110826092 \n",
      "Epoch:  34\n",
      "0/16.8125 loss: 0.0010512780863791704 \n",
      "Epoch:  34\n",
      "1/16.8125 loss: 0.0013156982604414225 \n",
      "Epoch:  34\n",
      "2/16.8125 loss: 0.0012676358067740996 \n",
      "Epoch:  34\n",
      "3/16.8125 loss: 0.0012164443905930966 \n",
      "Epoch:  34\n",
      "4/16.8125 loss: 0.0011771877529099584 \n",
      "Epoch:  34\n",
      "5/16.8125 loss: 0.001147216244135052 \n",
      "Epoch:  34\n",
      "6/16.8125 loss: 0.0011301140932898437 \n",
      "Epoch:  34\n",
      "7/16.8125 loss: 0.0011221767199458554 \n",
      "Epoch:  34\n",
      "8/16.8125 loss: 0.0011256406368273827 \n",
      "Epoch:  34\n",
      "9/16.8125 loss: 0.0011315151583403348 \n",
      "Epoch:  34\n",
      "10/16.8125 loss: 0.001116159964691509 \n",
      "Epoch:  34\n",
      "11/16.8125 loss: 0.0011060635539858292 \n",
      "Epoch:  34\n",
      "12/16.8125 loss: 0.0010962791006582288 \n",
      "Epoch:  34\n",
      "13/16.8125 loss: 0.0011325377771364792 \n",
      "Epoch:  34\n",
      "14/16.8125 loss: 0.0011336502541477481 \n",
      "Epoch:  34\n",
      "15/16.8125 loss: 0.0011188735807081684 \n",
      "Epoch:  34\n",
      "16/16.8125 loss: 0.0012828082750167918 \n",
      "Epoch:  35\n",
      "0/16.8125 loss: 0.0008240018505603075 \n",
      "Epoch:  35\n",
      "1/16.8125 loss: 0.0009659602073952556 \n",
      "Epoch:  35\n",
      "2/16.8125 loss: 0.0010533406166359782 \n",
      "Epoch:  35\n",
      "3/16.8125 loss: 0.0011563919833861291 \n",
      "Epoch:  35\n",
      "4/16.8125 loss: 0.0012936929007992149 \n",
      "Epoch:  35\n",
      "5/16.8125 loss: 0.0012367632104239117 \n",
      "Epoch:  35\n",
      "6/16.8125 loss: 0.0012267323992481188 \n",
      "Epoch:  35\n",
      "7/16.8125 loss: 0.0011967585815000348 \n",
      "Epoch:  35\n",
      "8/16.8125 loss: 0.0011718657204053467 \n",
      "Epoch:  35\n",
      "9/16.8125 loss: 0.0011444107978604734 \n",
      "Epoch:  35\n",
      "10/16.8125 loss: 0.0011234402614222331 \n",
      "Epoch:  35\n",
      "11/16.8125 loss: 0.0011064819118473679 \n",
      "Epoch:  35\n",
      "12/16.8125 loss: 0.001089795116478434 \n",
      "Epoch:  35\n",
      "13/16.8125 loss: 0.0012667460832744837 \n",
      "Epoch:  35\n",
      "14/16.8125 loss: 0.0012521180169035992 \n",
      "Epoch:  35\n",
      "15/16.8125 loss: 0.0012295742526475806 \n",
      "Epoch:  35\n",
      "16/16.8125 loss: 0.001212187559919103 \n",
      "Epoch:  36\n",
      "0/16.8125 loss: 0.000965747341979295 \n",
      "Epoch:  36\n",
      "1/16.8125 loss: 0.0012147149245720357 \n",
      "Epoch:  36\n",
      "2/16.8125 loss: 0.0011040042736567557 \n",
      "Epoch:  36\n",
      "3/16.8125 loss: 0.0010288156481692567 \n",
      "Epoch:  36\n",
      "4/16.8125 loss: 0.001047176227439195 \n",
      "Epoch:  36\n",
      "5/16.8125 loss: 0.0010074408492073417 \n",
      "Epoch:  36\n",
      "6/16.8125 loss: 0.0009929425848115767 \n",
      "Epoch:  36\n",
      "7/16.8125 loss: 0.0010030378180090338 \n",
      "Epoch:  36\n",
      "8/16.8125 loss: 0.0010001696258162458 \n",
      "Epoch:  36\n",
      "9/16.8125 loss: 0.0009764182614162564 \n",
      "Epoch:  36\n",
      "10/16.8125 loss: 0.0009782286043363538 \n",
      "Epoch:  36\n",
      "11/16.8125 loss: 0.0009744613101550689 \n",
      "Epoch:  36\n",
      "12/16.8125 loss: 0.0009631045037307418 \n",
      "Epoch:  36\n",
      "13/16.8125 loss: 0.0009602109618884112 \n",
      "Epoch:  36\n",
      "14/16.8125 loss: 0.0009513027306335668 \n",
      "Epoch:  36\n",
      "15/16.8125 loss: 0.0009865450592769776 \n",
      "Epoch:  36\n",
      "16/16.8125 loss: 0.00100294836481814 \n",
      "Epoch:  37\n",
      "0/16.8125 loss: 0.0008445469429716468 \n",
      "Epoch:  37\n",
      "1/16.8125 loss: 0.0008624879992567003 \n",
      "Epoch:  37\n",
      "2/16.8125 loss: 0.0009095584197590748 \n",
      "Epoch:  37\n",
      "3/16.8125 loss: 0.0009153103310381994 \n",
      "Epoch:  37\n",
      "4/16.8125 loss: 0.0009363811346702278 \n",
      "Epoch:  37\n",
      "5/16.8125 loss: 0.000928837563454484 \n",
      "Epoch:  37\n",
      "6/16.8125 loss: 0.0013042348686472646 \n",
      "Epoch:  37\n",
      "7/16.8125 loss: 0.0012454612806322984 \n",
      "Epoch:  37\n",
      "8/16.8125 loss: 0.0011966103338636458 \n",
      "Epoch:  37\n",
      "9/16.8125 loss: 0.0011626701802015305 \n",
      "Epoch:  37\n",
      "10/16.8125 loss: 0.0011323579714040864 \n",
      "Epoch:  37\n",
      "11/16.8125 loss: 0.0011042964518613492 \n",
      "Epoch:  37\n",
      "12/16.8125 loss: 0.0010807948464599366 \n",
      "Epoch:  37\n",
      "13/16.8125 loss: 0.010374739645547899 \n",
      "Epoch:  37\n",
      "14/16.8125 loss: 0.00973199283471331 \n",
      "Epoch:  37\n",
      "15/16.8125 loss: 0.009184338086924981 \n",
      "Epoch:  37\n",
      "16/16.8125 loss: 0.008705819833695012 \n",
      "Epoch:  38\n",
      "0/16.8125 loss: 0.0008084684377536178 \n",
      "Epoch:  38\n",
      "1/16.8125 loss: 0.0011931032058782876 \n",
      "Epoch:  38\n",
      "2/16.8125 loss: 0.0015095384248221915 \n",
      "Epoch:  38\n",
      "3/16.8125 loss: 0.0014917617663741112 \n",
      "Epoch:  38\n",
      "4/16.8125 loss: 0.0019326519686728716 \n",
      "Epoch:  38\n",
      "5/16.8125 loss: 0.0024144252529367805 \n",
      "Epoch:  38\n",
      "6/16.8125 loss: 0.02450910639683051 \n",
      "Epoch:  38\n",
      "7/16.8125 loss: 0.023702021717326716 \n",
      "Epoch:  38\n",
      "8/16.8125 loss: 0.021262920099414058 \n",
      "Epoch:  38\n",
      "9/16.8125 loss: 0.01931589434389025 \n",
      "Epoch:  38\n",
      "10/16.8125 loss: 0.01764752351234413 \n",
      "Epoch:  38\n",
      "11/16.8125 loss: 0.016270009262370877 \n",
      "Epoch:  38\n",
      "12/16.8125 loss: 0.015096202506146465 \n",
      "Epoch:  38\n",
      "13/16.8125 loss: 0.014097063099533054 \n",
      "Epoch:  38\n",
      "14/16.8125 loss: 0.013217168669992436 \n",
      "Epoch:  38\n",
      "15/16.8125 loss: 0.012546284528070828 \n",
      "Epoch:  38\n",
      "16/16.8125 loss: 0.012588229355648817 \n",
      "Epoch:  39\n",
      "0/16.8125 loss: 0.009560124948620796 \n",
      "Epoch:  39\n",
      "1/16.8125 loss: 0.005440406617708504 \n",
      "Epoch:  39\n",
      "2/16.8125 loss: 0.01656322934043904 \n",
      "Epoch:  39\n",
      "3/16.8125 loss: 0.012681347696343437 \n",
      "Epoch:  39\n",
      "4/16.8125 loss: 0.010373064060695469 \n",
      "Epoch:  39\n",
      "5/16.8125 loss: 0.008815815070799241 \n",
      "Epoch:  39\n",
      "6/16.8125 loss: 0.007703510013275913 \n",
      "Epoch:  39\n",
      "7/16.8125 loss: 0.006881297726067714 \n",
      "Epoch:  39\n",
      "8/16.8125 loss: 0.006308585845140947 \n",
      "Epoch:  39\n",
      "9/16.8125 loss: 0.013681988068856298 \n",
      "Epoch:  39\n",
      "10/16.8125 loss: 0.020087308864193885 \n",
      "Epoch:  39\n",
      "11/16.8125 loss: 0.01849684488843195 \n",
      "Epoch:  39\n",
      "12/16.8125 loss: 0.017140299547463655 \n",
      "Epoch:  39\n",
      "13/16.8125 loss: 0.015981832510858242 \n",
      "Epoch:  39\n",
      "14/16.8125 loss: 0.015159121023801465 \n",
      "Epoch:  39\n",
      "15/16.8125 loss: 0.01487513037136523 \n",
      "Epoch:  39\n",
      "16/16.8125 loss: 0.015322161502862239 \n",
      "Epoch:  40\n",
      "0/16.8125 loss: 0.0035669002681970596 \n",
      "Epoch:  40\n",
      "1/16.8125 loss: 0.0845179008319974 \n",
      "Epoch:  40\n",
      "2/16.8125 loss: 0.05669143404035518 \n",
      "Epoch:  40\n",
      "3/16.8125 loss: 0.042793752159923315 \n",
      "Epoch:  40\n",
      "4/16.8125 loss: 0.03446392505429685 \n",
      "Epoch:  40\n",
      "5/16.8125 loss: 0.04665244941134006 \n",
      "Epoch:  40\n",
      "6/16.8125 loss: 0.04035852027923933 \n",
      "Epoch:  40\n",
      "7/16.8125 loss: 0.03664734322228469 \n",
      "Epoch:  40\n",
      "8/16.8125 loss: 0.03290651829188897 \n",
      "Epoch:  40\n",
      "9/16.8125 loss: 0.030433607078157364 \n",
      "Epoch:  40\n",
      "10/16.8125 loss: 0.02775305739222941 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  40\n",
      "11/16.8125 loss: 0.02566571839270182 \n",
      "Epoch:  40\n",
      "12/16.8125 loss: 0.02376939217524173 \n",
      "Epoch:  40\n",
      "13/16.8125 loss: 0.022378644049500247 \n",
      "Epoch:  40\n",
      "14/16.8125 loss: 0.020968000784826777 \n",
      "Epoch:  40\n",
      "15/16.8125 loss: 0.019712388322659535 \n",
      "Epoch:  40\n",
      "16/16.8125 loss: 0.018600893087055096 \n",
      "Epoch:  41\n",
      "0/16.8125 loss: 0.001389227109029889 \n",
      "Epoch:  41\n",
      "1/16.8125 loss: 0.011850698501802981 \n",
      "Epoch:  41\n",
      "2/16.8125 loss: 0.00816595236149927 \n",
      "Epoch:  41\n",
      "3/16.8125 loss: 0.006464726640842855 \n",
      "Epoch:  41\n",
      "4/16.8125 loss: 0.0053470124490559105 \n",
      "Epoch:  41\n",
      "5/16.8125 loss: 0.004641369528447588 \n",
      "Epoch:  41\n",
      "6/16.8125 loss: 0.004081867236111846 \n",
      "Epoch:  41\n",
      "7/16.8125 loss: 0.003671333790407516 \n",
      "Epoch:  41\n",
      "8/16.8125 loss: 0.003427163038092355 \n",
      "Epoch:  41\n",
      "9/16.8125 loss: 0.0032023388426750897 \n",
      "Epoch:  41\n",
      "10/16.8125 loss: 0.0031745104034515944 \n",
      "Epoch:  41\n",
      "11/16.8125 loss: 0.009762150739940504 \n",
      "Epoch:  41\n",
      "12/16.8125 loss: 0.009071809884447318 \n",
      "Epoch:  41\n",
      "13/16.8125 loss: 0.00848140484387321 \n",
      "Epoch:  41\n",
      "14/16.8125 loss: 0.007990876771509647 \n",
      "Epoch:  41\n",
      "15/16.8125 loss: 0.008088528935331851 \n",
      "Epoch:  41\n",
      "16/16.8125 loss: 0.008482420926584917 \n",
      "Epoch:  42\n",
      "0/16.8125 loss: 0.0014341584173962474 \n",
      "Epoch:  42\n",
      "1/16.8125 loss: 0.0012292893952690065 \n",
      "Epoch:  42\n",
      "2/16.8125 loss: 0.0012010935073097546 \n",
      "Epoch:  42\n",
      "3/16.8125 loss: 0.0012341994734015316 \n",
      "Epoch:  42\n",
      "4/16.8125 loss: 0.0011636525276117026 \n",
      "Epoch:  42\n",
      "5/16.8125 loss: 0.001258568576304242 \n",
      "Epoch:  42\n",
      "6/16.8125 loss: 0.0030214143002272715 \n",
      "Epoch:  42\n",
      "7/16.8125 loss: 0.0027867564422194846 \n",
      "Epoch:  42\n",
      "8/16.8125 loss: 0.002574215636640373 \n",
      "Epoch:  42\n",
      "9/16.8125 loss: 0.002401809551520273 \n",
      "Epoch:  42\n",
      "10/16.8125 loss: 0.002256432939744131 \n",
      "Epoch:  42\n",
      "11/16.8125 loss: 0.002155074510180081 \n",
      "Epoch:  42\n",
      "12/16.8125 loss: 0.0020827691769227386 \n",
      "Epoch:  42\n",
      "13/16.8125 loss: 0.002018339516195868 \n",
      "Epoch:  42\n",
      "14/16.8125 loss: 0.0019635457235078015 \n",
      "Epoch:  42\n",
      "15/16.8125 loss: 0.0018923690113297198 \n",
      "Epoch:  42\n",
      "16/16.8125 loss: 0.0019321021219879826 \n",
      "Epoch:  43\n",
      "0/16.8125 loss: 0.0007684353622607887 \n",
      "Epoch:  43\n",
      "1/16.8125 loss: 0.0008014278719201684 \n",
      "Epoch:  43\n",
      "2/16.8125 loss: 0.0010171513616417844 \n",
      "Epoch:  43\n",
      "3/16.8125 loss: 0.0016595226188655943 \n",
      "Epoch:  43\n",
      "4/16.8125 loss: 0.0014852992026135325 \n",
      "Epoch:  43\n",
      "5/16.8125 loss: 0.001412964581201474 \n",
      "Epoch:  43\n",
      "6/16.8125 loss: 0.0013503850121716304 \n",
      "Epoch:  43\n",
      "7/16.8125 loss: 0.0012651996948989108 \n",
      "Epoch:  43\n",
      "8/16.8125 loss: 0.0012089181109331548 \n",
      "Epoch:  43\n",
      "9/16.8125 loss: 0.0012226573249790817 \n",
      "Epoch:  43\n",
      "10/16.8125 loss: 0.0011806253089823506 \n",
      "Epoch:  43\n",
      "11/16.8125 loss: 0.0011398736969567835 \n",
      "Epoch:  43\n",
      "12/16.8125 loss: 0.0011077069614727336 \n",
      "Epoch:  43\n",
      "13/16.8125 loss: 0.0011555453571158328 \n",
      "Epoch:  43\n",
      "14/16.8125 loss: 0.001170486231179287 \n",
      "Epoch:  43\n",
      "15/16.8125 loss: 0.0011523518442118075 \n",
      "Epoch:  43\n",
      "16/16.8125 loss: 0.001143741061883595 \n",
      "Epoch:  44\n",
      "0/16.8125 loss: 0.0008996833930723369 \n",
      "Epoch:  44\n",
      "1/16.8125 loss: 0.0010060717177111655 \n",
      "Epoch:  44\n",
      "2/16.8125 loss: 0.0008858135746171077 \n",
      "Epoch:  44\n",
      "3/16.8125 loss: 0.0011310043919365853 \n",
      "Epoch:  44\n",
      "4/16.8125 loss: 0.0012503529665991663 \n",
      "Epoch:  44\n",
      "5/16.8125 loss: 0.0011821269581560045 \n",
      "Epoch:  44\n",
      "6/16.8125 loss: 0.00110731807736946 \n",
      "Epoch:  44\n",
      "7/16.8125 loss: 0.0010487866602488793 \n",
      "Epoch:  44\n",
      "8/16.8125 loss: 0.0010138868625896673 \n",
      "Epoch:  44\n",
      "9/16.8125 loss: 0.0009857051714789121 \n",
      "Epoch:  44\n",
      "10/16.8125 loss: 0.001329590793995356 \n",
      "Epoch:  44\n",
      "11/16.8125 loss: 0.0012914073837843414 \n",
      "Epoch:  44\n",
      "12/16.8125 loss: 0.0026168051990680397 \n",
      "Epoch:  44\n",
      "13/16.8125 loss: 0.002479582708994193 \n",
      "Epoch:  44\n",
      "14/16.8125 loss: 0.002357687874852369 \n",
      "Epoch:  44\n",
      "15/16.8125 loss: 0.0022562170051969588 \n",
      "Epoch:  44\n",
      "16/16.8125 loss: 0.0022630017962964144 \n",
      "Epoch:  45\n",
      "0/16.8125 loss: 0.0008348958217538893 \n",
      "Epoch:  45\n",
      "1/16.8125 loss: 0.0024131771933753043 \n",
      "Epoch:  45\n",
      "2/16.8125 loss: 0.0019209354844254751 \n",
      "Epoch:  45\n",
      "3/16.8125 loss: 0.001743363609421067 \n",
      "Epoch:  45\n",
      "4/16.8125 loss: 0.0015629992354661225 \n",
      "Epoch:  45\n",
      "5/16.8125 loss: 0.022805540550810594 \n",
      "Epoch:  45\n",
      "6/16.8125 loss: 0.019686252535653433 \n",
      "Epoch:  45\n",
      "7/16.8125 loss: 0.017293736171268392 \n",
      "Epoch:  45\n",
      "8/16.8125 loss: 0.01543435614116283 \n",
      "Epoch:  45\n",
      "9/16.8125 loss: 0.013959391607204451 \n",
      "Epoch:  45\n",
      "10/16.8125 loss: 0.012755857283164833 \n",
      "Epoch:  45\n",
      "11/16.8125 loss: 0.011828598178302249 \n",
      "Epoch:  45\n",
      "12/16.8125 loss: 0.011147848079697443 \n",
      "Epoch:  45\n",
      "13/16.8125 loss: 0.0104080164331078 \n",
      "Epoch:  45\n",
      "14/16.8125 loss: 0.01014927145636951 \n",
      "Epoch:  45\n",
      "15/16.8125 loss: 0.009932497439876897 \n",
      "Epoch:  45\n",
      "16/16.8125 loss: 0.04161864336621126 \n",
      "Epoch:  46\n",
      "0/16.8125 loss: 0.00312402518466115 \n",
      "Epoch:  46\n",
      "1/16.8125 loss: 0.1243704988155514 \n",
      "Epoch:  46\n",
      "2/16.8125 loss: 0.1149905586304764 \n",
      "Epoch:  46\n",
      "3/16.8125 loss: 0.08648843626724556 \n",
      "Epoch:  46\n",
      "4/16.8125 loss: 0.06962300427258014 \n",
      "Epoch:  46\n",
      "5/16.8125 loss: 0.05819841497577727 \n",
      "Epoch:  46\n",
      "6/16.8125 loss: 0.051186162778841596 \n",
      "Epoch:  46\n",
      "7/16.8125 loss: 0.04905557614983991 \n",
      "Epoch:  46\n",
      "8/16.8125 loss: 0.04478683793503377 \n",
      "Epoch:  46\n",
      "9/16.8125 loss: 0.0449274571146816 \n",
      "Epoch:  46\n",
      "10/16.8125 loss: 0.0465708674270321 \n",
      "Epoch:  46\n",
      "11/16.8125 loss: 0.044007701760468386 \n",
      "Epoch:  46\n",
      "12/16.8125 loss: 0.04090877821167501 \n",
      "Epoch:  46\n",
      "13/16.8125 loss: 0.03808992978883907 \n",
      "Epoch:  46\n",
      "14/16.8125 loss: 0.03570380652478586 \n",
      "Epoch:  46\n",
      "15/16.8125 loss: 0.03359652788640233 \n",
      "Epoch:  46\n",
      "16/16.8125 loss: 0.033466044272405696 \n",
      "Epoch:  47\n",
      "0/16.8125 loss: 0.0034442725591361523 \n",
      "Epoch:  47\n",
      "1/16.8125 loss: 0.025084795197471976 \n",
      "Epoch:  47\n",
      "2/16.8125 loss: 0.01952722715213895 \n",
      "Epoch:  47\n",
      "3/16.8125 loss: 0.015121582720894367 \n",
      "Epoch:  47\n",
      "4/16.8125 loss: 0.016931895399466157 \n",
      "Epoch:  47\n",
      "5/16.8125 loss: 0.01471921611422052 \n",
      "Epoch:  47\n",
      "6/16.8125 loss: 0.012838994751551322 \n",
      "Epoch:  47\n",
      "7/16.8125 loss: 0.01258249202510342 \n",
      "Epoch:  47\n",
      "8/16.8125 loss: 0.011371104266597994 \n",
      "Epoch:  47\n",
      "9/16.8125 loss: 0.01051384083693847 \n",
      "Epoch:  47\n",
      "10/16.8125 loss: 0.009761298826726323 \n",
      "Epoch:  47\n",
      "11/16.8125 loss: 0.009148852831761664 \n",
      "Epoch:  47\n",
      "12/16.8125 loss: 0.02431408080379837 \n",
      "Epoch:  47\n",
      "13/16.8125 loss: 0.022686739518706287 \n",
      "Epoch:  47\n",
      "14/16.8125 loss: 0.02135625327937305 \n",
      "Epoch:  47\n",
      "15/16.8125 loss: 0.02011274337564828 \n",
      "Epoch:  47\n",
      "16/16.8125 loss: 0.019018916016006294 \n",
      "Epoch:  48\n",
      "0/16.8125 loss: 0.0020781445782631636 \n",
      "Epoch:  48\n",
      "1/16.8125 loss: 0.005738763953559101 \n",
      "Epoch:  48\n",
      "2/16.8125 loss: 0.004193262313492596 \n",
      "Epoch:  48\n",
      "3/16.8125 loss: 0.008671698946272954 \n",
      "Epoch:  48\n",
      "4/16.8125 loss: 0.007421595067717135 \n",
      "Epoch:  48\n",
      "5/16.8125 loss: 0.0064478165392453475 \n",
      "Epoch:  48\n",
      "6/16.8125 loss: 0.005695264936158699 \n",
      "Epoch:  48\n",
      "7/16.8125 loss: 0.005136451116413809 \n",
      "Epoch:  48\n",
      "8/16.8125 loss: 0.004692536481242213 \n",
      "Epoch:  48\n",
      "9/16.8125 loss: 0.00442359036533162 \n",
      "Epoch:  48\n",
      "10/16.8125 loss: 0.004161338628777726 \n",
      "Epoch:  48\n",
      "11/16.8125 loss: 0.003926627037193005 \n",
      "Epoch:  48\n",
      "12/16.8125 loss: 0.003721621702425182 \n",
      "Epoch:  48\n",
      "13/16.8125 loss: 0.0035533906941834304 \n",
      "Epoch:  48\n",
      "14/16.8125 loss: 0.003490874784377714 \n",
      "Epoch:  48\n",
      "15/16.8125 loss: 0.003360000417160336 \n",
      "Epoch:  48\n",
      "16/16.8125 loss: 0.0032266919802436058 \n",
      "Epoch:  49\n",
      "0/16.8125 loss: 0.0011078796815127134 \n",
      "Epoch:  49\n",
      "1/16.8125 loss: 0.0010499914642423391 \n",
      "Epoch:  49\n",
      "2/16.8125 loss: 0.0011378904261315863 \n",
      "Epoch:  49\n",
      "3/16.8125 loss: 0.001268628693651408 \n",
      "Epoch:  49\n",
      "4/16.8125 loss: 0.0011784070869907738 \n",
      "Epoch:  49\n",
      "5/16.8125 loss: 0.001118034958684196 \n",
      "Epoch:  49\n",
      "6/16.8125 loss: 0.001126727372008775 \n",
      "Epoch:  49\n",
      "7/16.8125 loss: 0.002316738376975991 \n",
      "Epoch:  49\n",
      "8/16.8125 loss: 0.0021730552091159755 \n",
      "Epoch:  49\n",
      "9/16.8125 loss: 0.002051935065537691 \n",
      "Epoch:  49\n",
      "10/16.8125 loss: 0.00197276451878927 \n",
      "Epoch:  49\n",
      "11/16.8125 loss: 0.0019009877578355372 \n",
      "Epoch:  49\n",
      "12/16.8125 loss: 0.0018217791131554316 \n",
      "Epoch:  49\n",
      "13/16.8125 loss: 0.0018674955811418062 \n",
      "Epoch:  49\n",
      "14/16.8125 loss: 0.0018748931935988367 \n",
      "Epoch:  49\n",
      "15/16.8125 loss: 0.001812903592508519 \n",
      "Epoch:  49\n",
      "16/16.8125 loss: 0.0017521734732915374 \n",
      "Epoch:  50\n",
      "0/16.8125 loss: 0.0009332429035566747 \n",
      "Epoch:  50\n",
      "1/16.8125 loss: 0.0008827195269986987 \n",
      "Epoch:  50\n",
      "2/16.8125 loss: 0.0009518694132566452 \n",
      "Epoch:  50\n",
      "3/16.8125 loss: 0.004465894540771842 \n",
      "Epoch:  50\n",
      "4/16.8125 loss: 0.0037679151748307048 \n",
      "Epoch:  50\n",
      "5/16.8125 loss: 0.0032891499577090144 \n",
      "Epoch:  50\n",
      "6/16.8125 loss: 0.0029456180553617223 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  50\n",
      "7/16.8125 loss: 0.0026874718387261964 \n",
      "Epoch:  50\n",
      "8/16.8125 loss: 0.00249388350898193 \n",
      "Epoch:  50\n",
      "9/16.8125 loss: 0.002400051616132259 \n",
      "Epoch:  50\n",
      "10/16.8125 loss: 0.0022606832102279773 \n",
      "Epoch:  50\n",
      "11/16.8125 loss: 0.0021533336548600346 \n",
      "Epoch:  50\n",
      "12/16.8125 loss: 0.0022305699441438685 \n",
      "Epoch:  50\n",
      "13/16.8125 loss: 0.0022063928855849163 \n",
      "Epoch:  50\n",
      "14/16.8125 loss: 0.002112075213032464 \n",
      "Epoch:  50\n",
      "15/16.8125 loss: 0.002095687021210324 \n",
      "Epoch:  50\n",
      "16/16.8125 loss: 0.002032319172833334 \n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_texts) / BATCH_SIZE, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "      \n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "\n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For:  Feminist Movement               precision    recall  f1-score   support\n",
      "\n",
      "       False     0.7958    0.8306    0.8128       183\n",
      "        True     0.3800    0.3276    0.3519        58\n",
      "\n",
      "    accuracy                         0.7095       241\n",
      "   macro avg     0.5879    0.5791    0.5823       241\n",
      "weighted avg     0.6957    0.7095    0.7019       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For: \",t,classification_report(test_y, bert_predicted,digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve\n",
    "# import sklearn\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# fpr, tpr, threshold = roc_curve(y_test, y_predict)\n",
    "# roc_auc = sklearn.metrics.auc(_fpr, _tpr)\n",
    "# f1_score = f1_score(y_test, y_predict)\n",
    "# print('AUROC: {}'.format(roc_auc), file=open(\"output.txt\",\"a\"))\n",
    "# print('f1-score: {}'.format(f1_score), file=open(\"output.txt\",\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
