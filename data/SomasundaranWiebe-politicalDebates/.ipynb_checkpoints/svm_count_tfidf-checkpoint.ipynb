{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wMrwqRLs91Vf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "#from google.colab import drive\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guwz2mom9-RN",
    "outputId": "d90bc4e9-cc1c-43ce-d5f6-a79ac5066271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train data is 4991\n",
      "The length of test data is 2143\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "df = pd.read_csv('train.txt', sep='\\t')\n",
    "df1 = pd.read_csv('test.txt', sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "#Use below lines only when training different classifiers for different targerts and testing on their corrosponding data.\n",
    "# t = ['abortion', 'creation', 'gayRights', 'god', 'guns', 'healthcare']\n",
    "# df = df[(df[\"Target\"]==t[4]) ]\n",
    "# df1 = df1[(df1[\"Target\"]==t[4]) ]\n",
    "\n",
    "print(\"The length of train data is {}\".format(len(df)))\n",
    "print(\"The length of test data is {}\".format(len(df1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abortion', 'creation', 'gayRights', 'god', 'guns', 'healthcare'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = 'tfidf'   # set 'count' or 'tfidf'\n",
    "analyzer = 'both'  # set 'word' or 'both' ( word and char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu37tw9Vmm3p",
    "outputId": "497cfeae-f6d3-4588-b465-80ece0117023"
   },
   "outputs": [],
   "source": [
    "if vectorizer == 'count':\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = CountVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "else:\n",
    "    if analyzer == 'word':\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\n",
    "        char_vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(2,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5QKhyyn3GW3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "pqbMqivk-EBF"
   },
   "outputs": [],
   "source": [
    "#List of FAVOR Tweets\n",
    "def get_training_data_and_labels(df):\n",
    "    df_train_favor = df.loc[df['Stance'] == 'FAVOR']\n",
    "    df_train_favor = df_train_favor.reset_index(drop=True)\n",
    "    train_favor_tweets = df_train_favor['Tweet'].tolist()\n",
    "    \n",
    "    # List of AGAINST Tweets\n",
    "    df_train_against = df.loc[df['Stance'] == 'AGAINST']\n",
    "    df_train_against = df_train_against.reset_index(drop=True)\n",
    "    train_against_tweets = df_train_against['Tweet'].tolist()\n",
    "    \n",
    "    #Favor + Against Tweets and Labels\n",
    "    train_corpus = train_favor_tweets + train_against_tweets\n",
    "    train_labels = np.append(np.ones((len(train_favor_tweets))) , np.zeros((len(train_against_tweets))))\n",
    "    \n",
    "    \n",
    "    if analyzer == 'word':\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        \n",
    "        return ngram_vectorized_data, train_labels\n",
    "    else:\n",
    "        ngram_vectorized_data = vectorizer.fit_transform(train_corpus)\n",
    "        char_vectorized_data = char_vectorizer.fit_transform(train_corpus)\n",
    "        l = np.hstack((ngram_vectorized_data.toarray(), char_vectorized_data.toarray()))\n",
    "        train_vectorized_data = sparse.csr_matrix(l)\n",
    "        \n",
    "        return train_vectorized_data, train_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2QL4bjHgloM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "rKNZ6AA4G9Tv"
   },
   "outputs": [],
   "source": [
    "#preparing test_data\n",
    "def get_test_data_and_labels(df1):\n",
    "    df_test_favor = df1.loc[df1['Stance']=='FAVOR']\n",
    "    df_test_favor = df_test_favor.reset_index(drop=True)\n",
    "    test_favor_tweets = df_test_favor['Tweet'].tolist()\n",
    "    print(len(test_favor_tweets))\n",
    "    \n",
    "    \n",
    "    df_test_against = df1.loc[df1['Stance'] == 'AGAINST']\n",
    "    df_test_against = df_test_against.reset_index(drop=True)\n",
    "    test_against_tweets = df_test_against['Tweet'].tolist()\n",
    "    print(len(test_against_tweets))\n",
    "    \n",
    "    \n",
    "    test_corpus = test_favor_tweets + test_against_tweets\n",
    "    test_labels = np.append(np.ones((len(test_favor_tweets))) , np.zeros((len(test_against_tweets))))\n",
    "    \n",
    "    if analyzer == 'word':\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        \n",
    "        return test_ngram_vectorized_data, test_labels\n",
    "    else:\n",
    "        test_ngram_vectorized_data = vectorizer.transform(test_corpus)\n",
    "        test_char_vectorized_data = char_vectorizer.transform(test_corpus)\n",
    "        l2 = np.hstack((test_ngram_vectorized_data.toarray(), test_char_vectorized_data.toarray()))\n",
    "        test_vectorized_data = sparse.csr_matrix(l2)\n",
    "        \n",
    "        return test_vectorized_data,test_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBQR_GZS-Zwd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgrH2FsLNu8B",
    "outputId": "577a1218-22f0-4183-b362-09a027f2fe2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n",
      "715\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train =  get_training_data_and_labels(df)\n",
    "X_test, y_test = get_test_data_and_labels(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tR4ULhqpQR_h",
    "outputId": "5866db2f-b8f4-46a1-a353-8a1ac3a616db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/p/parush/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.325 (+/-0.001) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.325 (+/-0.001) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.325 (+/-0.001) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.325 (+/-0.001) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.525 (+/-0.492) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.325 (+/-0.001) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.624 (+/-0.105) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.525 (+/-0.492) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.627 (+/-0.109) for {'C': 1, 'kernel': 'linear'}\n",
      "0.624 (+/-0.107) for {'C': 10, 'kernel': 'linear'}\n",
      "0.624 (+/-0.107) for {'C': 100, 'kernel': 'linear'}\n",
      "0.624 (+/-0.107) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       715\n",
      "         1.0       0.59      0.63      0.61       304\n",
      "\n",
      "    accuracy                           0.76      1019\n",
      "   macro avg       0.72      0.72      0.72      1019\n",
      "weighted avg       0.76      0.76      0.76      1019\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.502 (+/-0.015) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.572 (+/-0.036) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.502 (+/-0.015) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.572 (+/-0.036) for {'C': 1, 'kernel': 'linear'}\n",
      "0.572 (+/-0.037) for {'C': 10, 'kernel': 'linear'}\n",
      "0.572 (+/-0.037) for {'C': 100, 'kernel': 'linear'}\n",
      "0.572 (+/-0.037) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       715\n",
      "         1.0       0.59      0.63      0.61       304\n",
      "\n",
      "    accuracy                           0.76      1019\n",
      "   macro avg       0.72      0.72      0.72      1019\n",
      "weighted avg       0.76      0.76      0.76      1019\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPmjri5VbvZP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryWZpF-zxohF",
    "outputId": "54ddc458-facf-4811-cf86-7aa438113765"
   },
   "outputs": [],
   "source": [
    "# df_all = pd.read_csv('test.csv')\n",
    "# X_test, y_test = get_test_data_and_labels(df_all)\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "# print('Report for ', classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCebQWZuqO3U",
    "outputId": "b36a171a-b1b7-4737-e616-887c412aa8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "160\n",
      "Report for  Atheism               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.91      0.89       160\n",
      "         1.0       0.42      0.34      0.38        32\n",
      "\n",
      "    accuracy                           0.81       192\n",
      "   macro avg       0.65      0.62      0.63       192\n",
      "weighted avg       0.80      0.81      0.80       192\n",
      "\n",
      "123\n",
      "11\n",
      "Report for  Climate Change is a Real Concern               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.19      0.55      0.29        11\n",
      "         1.0       0.95      0.80      0.87       123\n",
      "\n",
      "    accuracy                           0.78       134\n",
      "   macro avg       0.57      0.67      0.58       134\n",
      "weighted avg       0.89      0.78      0.82       134\n",
      "\n",
      "58\n",
      "183\n",
      "Report for  Feminist Movement               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.73      0.79       183\n",
      "         1.0       0.42      0.60      0.49        58\n",
      "\n",
      "    accuracy                           0.70       241\n",
      "   macro avg       0.64      0.67      0.64       241\n",
      "weighted avg       0.75      0.70      0.72       241\n",
      "\n",
      "45\n",
      "172\n",
      "Report for  Hillary Clinton               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.87      0.85       172\n",
      "         1.0       0.42      0.36      0.39        45\n",
      "\n",
      "    accuracy                           0.76       217\n",
      "   macro avg       0.63      0.61      0.62       217\n",
      "weighted avg       0.75      0.76      0.76       217\n",
      "\n",
      "46\n",
      "189\n",
      "Report for  Legalization of Abortion               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.79      0.84       189\n",
      "         1.0       0.44      0.67      0.53        46\n",
      "\n",
      "    accuracy                           0.77       235\n",
      "   macro avg       0.67      0.73      0.69       235\n",
      "weighted avg       0.82      0.77      0.78       235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = ['Atheism', 'Climate Change is a Real Concern', 'Feminist Movement', 'Hillary Clinton', 'Legalization of Abortion']\n",
    "\n",
    "for target in t:\n",
    "    df_test = df1[(df1[\"Target\"]== target) ]\n",
    "    X_test, y_test = get_test_data_and_labels(df_test)\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print('Report for ',target, classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tkt-iBNjQ2WA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhFyOPDNaKFW"
   },
   "outputs": [],
   "source": [
    "#print(len(test_count_vectorizer.get_feature_names()))\n",
    "\n",
    "#test_indexed_data = hstack((np.array(range(0,test_vectorized_data.shape[0]))[:,None], test_vectorized_data)) #adding a column for index and stacking data 3614 X 100285\n",
    "#test_indexed_data.shape\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(indexed_data, labels, test_size=0.4, random_state=0, shuffle = True)\n",
    "#X_train,, y_train = indexed_data , labels\n",
    "#data_train_index = X_train[:,0]\n",
    "#print(X_test.shape)\n",
    "#print(data_train_index)\n",
    "#X_train = X_train[:,1:]\n",
    "#data_test_index = X_test[:,0]\n",
    "#print(data_test_index)\n",
    "#X_test = X_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrqI0RAdxeSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKwr73fHxe_G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Count_Stance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
