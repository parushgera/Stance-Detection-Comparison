{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples \n",
    "from utils import *\n",
    "#from parameters import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout,Concatenate,Dense, Embedding, LSTM, SpatialDropout1D, Flatten, GRU, Bidirectional, Conv1D, Input,MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5280"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_file = open(\"vocabulary.json\", \"r\")\n",
    "vocabulary = json.load(a_file)\n",
    "len(vocabulary['mohammed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = '/home/parush/stance/Experiments/stance_mohammed/train.txt'\n",
    "test_data_file = '/home/parush/stance/Experiments/stance_mohammed/test.txt'\n",
    "TARGETS = [ 'Atheism','Climate Change is a Real Concern', 'Feminist Movement','Hillary Clinton', 'Legalization of Abortion' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_file = '/home/parush/stance/Experiments/SomasundaranWiebe-politicalDebates/train.txt'\n",
    "# test_data_file = '/home/parush/stance/Experiments/SomasundaranWiebe-politicalDebates/test.txt'\n",
    "# TARGETS = ['god','healthcare','guns','gayRights','abortion', 'creation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_file = '/home/parush/stance/Experiments/Data_MPCHI/train.txt'\n",
    "# test_data_file = '/home/parush/stance/Experiments/Data_MPCHI/test.txt'\n",
    "# TARGETS = ['Are E-Cigarettes safe?','Does MMR Vaccine lead to autism in children?',\n",
    "#      'Does Sunlight exposure lead to skin cancer?','Does Vitamin C prevent common cold?',\n",
    "#      'Should women take HRT post-menopause?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    ### START CODE HERE ###\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    ### END CODE HERE ###\n",
    "    return tweets_clean\n",
    "\n",
    "def train_and_test():\n",
    "    \n",
    "    sentence_maxlen = 909 #set 909 only when loading biGRUCNN or biLSTMCNN trained on debates set.\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    all_favor_tweets = []\n",
    "    all_against_tweets = []\n",
    "    \n",
    "    with open(train_data_file, 'r') as trainfile:\n",
    "        for line in trainfile:\n",
    "            \n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "            \n",
    "            if line[0].strip() != 'ID' and line[3].strip() == 'FAVOR':\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_favor_tweets.append(tweet)\n",
    "            elif line[0].strip() != 'ID' and line[3].strip() == 'AGAINST':\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_against_tweets.append(tweet)\n",
    "            \n",
    "    x_train = all_favor_tweets + all_against_tweets\n",
    "    y_train = np.append(np.ones(len(all_favor_tweets)), np.zeros(len(all_against_tweets))) \n",
    "    \n",
    "    \n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    all_favor_tweets_test = []\n",
    "    all_against_tweets_test = []\n",
    "    with open(test_data_file, 'r') as testfile:\n",
    "        for line in testfile:\n",
    "            line = line.replace('#SemST', '').strip()\n",
    "            line = line.split('\\t')\n",
    "        \n",
    "\n",
    "            if line[0] != 'ID' and line[3] == 'FAVOR':\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_favor_tweets_test.append(tweet)\n",
    "            elif line[0].strip() != 'ID' and line[3].strip() == 'AGAINST':\n",
    "                tweet = line[2]\n",
    "                tweet = process_tweet(tweet)\n",
    "                if len(tweet) > sentence_maxlen:\n",
    "                    sentence_maxlen = len(tweet)\n",
    "                all_against_tweets_test.append(tweet)\n",
    "\n",
    "    x_test = all_favor_tweets_test + all_against_tweets_test\n",
    "    y_test = np.append(np.ones(len(all_favor_tweets_test)), np.zeros(len(all_against_tweets_test)))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, sentence_maxlen\n",
    "                \n",
    "            \n",
    "def tweet_to_tensor(processed_tweet, vocab_dict, unk_token=\"__UNK__\"):\n",
    "    tensor = []\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    for word in processed_tweet:\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "        tensor.append(word_ID)\n",
    "    return tensor                \n",
    "            \n",
    "def load_embeddings(embedding,dim):\n",
    "    if embedding == 'twitter':\n",
    "        path = '/home/parush/stance/Experiments/embeddings/twitter/glove.twitter.27B.'+str(dim)+'d.txt'\n",
    "    else:\n",
    "        path ='/home/parush/stance/Experiments/embeddings/wikipedia/glove.6B.'+str(dim)+'d.txt'\n",
    "    word_embeddings = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for each_emb in f:\n",
    "            emb = each_emb.split(' ')\n",
    "            word_embeddings[emb[0]] = np.asarray(emb[1:], dtype='float32')\n",
    "    return word_embeddings\n",
    "def get_embeddings(embedding,dim):\n",
    "    if embedding == 'twitter':\n",
    "        embedding_matrix_twitter = np.zeros((vocab_size, dim))\n",
    "        word_embeddings_twitter = load_embeddings(embedding, dim)\n",
    "        print(embedding_matrix_twitter[0])\n",
    "        for each_word,index in Vocab.items():\n",
    "            if each_word in word_embeddings_twitter:\n",
    "                embedding_matrix_twitter[index] = word_embeddings_twitter[each_word]\n",
    "        return embedding_matrix_twitter\n",
    "    else:\n",
    "        embedding_matrix_wikipedia = np.zeros((vocab_size, dim))\n",
    "        word_embeddings_wikipedia = load_embeddings(embedding, dim)\n",
    "        for each_word,index in Vocab.items():\n",
    "            if each_word in word_embeddings_wikipedia:\n",
    "                embedding_matrix_wikipedia[index] = word_embeddings_wikipedia[each_word]\n",
    "        return embedding_matrix_wikipedia\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, sentence_maxlen = train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    tweet_tensor = tweet_to_tensor(x_train[i], vocabulary['debates']) # Set dic of dataset of trained model that you will load\n",
    "    if len(tweet_tensor) < sentence_maxlen:\n",
    "        diff = sentence_maxlen - len(tweet_tensor)\n",
    "        n_pad = [0]*diff\n",
    "        tweet_tensor = tweet_tensor + n_pad\n",
    "    x_train[i] = tweet_tensor\n",
    "for i in range(len(x_test)):\n",
    "    tweet_tensor = tweet_to_tensor(x_test[i], vocabulary['debates']) # Set dic of dataset of trained model that you will load\n",
    "    if len(tweet_tensor) < sentence_maxlen:\n",
    "        diff = sentence_maxlen - len(tweet_tensor)\n",
    "        n_pad = [0]*diff\n",
    "        tweet_tensor = tweet_tensor + n_pad\n",
    "    x_test[i] = tweet_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    " \n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence maxlength 909,\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence maxlength {},\".format(sentence_maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019, 909)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = tf.keras.models.load_model('saved_model/debates/biLSTMCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7083    0.3329    0.4529       715\n",
      "         1.0     0.3016    0.6776    0.4174       304\n",
      "\n",
      "    accuracy                         0.4357      1019\n",
      "   macro avg     0.5050    0.5052    0.4352      1019\n",
      "weighted avg     0.5870    0.4357    0.4423      1019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(s_model.predict(x_test))\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 3s - loss: 0.6329 - accuracy: 0.6624\n",
      "accuracy: 72.76%\n",
      "108/108 - 3s - loss: 0.5180 - accuracy: 0.7595\n",
      "accuracy: 77.56%\n",
      "108/108 - 3s - loss: 0.3952 - accuracy: 0.8272\n",
      "accuracy: 87.44%\n",
      "108/108 - 3s - loss: 0.2934 - accuracy: 0.8889\n",
      "accuracy: 92.46%\n",
      "108/108 - 3s - loss: 0.2088 - accuracy: 0.9241\n",
      "accuracy: 95.44%\n",
      "85.13% (+/- 8.67%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, val in kfold.split(x_train, y_train):\n",
    "    \n",
    "    \n",
    "    model.fit(x_train[train], y_train[train], epochs = 1, batch_size = 16, verbose=2)\n",
    "    scores = model.evaluate(x_train[val], y_train[val], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4430    0.7688    0.5621       930\n",
      "         1.0     0.5897    0.2558    0.3568      1208\n",
      "\n",
      "    accuracy                         0.4790      2138\n",
      "   macro avg     0.5163    0.5123    0.4595      2138\n",
      "weighted avg     0.5259    0.4790    0.4461      2138\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
